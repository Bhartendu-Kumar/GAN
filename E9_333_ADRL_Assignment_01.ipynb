{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_0H8_EL31cH8",
        "Zqh60mQw1hon",
        "cq6Hd1sy2olg"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**E9 333 Advanced Deep Representation Learning (2022)**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Assignment 01\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Submission by: Dhruv Bhardwaj (SR 19280), Bhartendu Kumar (SR 19649), MTech AI"
      ],
      "metadata": {
        "id": "731sZLG_56aQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variational Auto Encoders (VAE)"
      ],
      "metadata": {
        "id": "YJX_y-pm5VYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standard Imports"
      ],
      "metadata": {
        "id": "j-X6UACH50Z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.manual_seed(42)\n",
        "\n",
        "from torch.nn import Conv2d, ConvTranspose2d, Linear, Embedding\n",
        "from torch.nn import MaxPool2d, BatchNorm2d\n",
        "from torch.nn import LeakyReLU, Tanh, ReLU, Sigmoid\n",
        "from torch.nn import Module\n",
        "from torch.nn import MSELoss\n",
        "from torch import flatten\n",
        "import numpy\n",
        "import random\n",
        "import os, os.path\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.io import read_image\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    numpy.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "    return \n",
        "\n",
        "g = torch.Generator()\n",
        "g.manual_seed(42)"
      ],
      "metadata": {
        "id": "jht8aSxi2dpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configurations for different tasks"
      ],
      "metadata": {
        "id": "lB9ZsZWU1Sx5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vanilla VAE - CelebA Dataset"
      ],
      "metadata": {
        "id": "_0H8_EL31cH8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Seca2Pj1CFK"
      },
      "outputs": [],
      "source": [
        "DATA_PATH   =   './datasets/img_align_celeba_resampled/'\n",
        "FILE_EXTN   =   '.jpg'\n",
        "SAVE_PATH   =   './logs/'\n",
        "BATCH_SIZE  =   100\n",
        "LEARN_RATE  =   0.001\n",
        "EPOCHS      =   100\n",
        "\n",
        "INPUT_H     = 64\n",
        "INPUT_W     = 64\n",
        "INPUT_CH    = 3\n",
        "\n",
        "LATENT_DIM  = 128\n",
        "NUM_GENERATE_SAMPLES    =   100\n",
        "DATA_VAR = 0.09"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vanilla VAE - dsprites Dataset"
      ],
      "metadata": {
        "id": "Zqh60mQw1hon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH   =   './datasets/dsprites/'\n",
        "FILE_EXTN   =   '.jpg'\n",
        "SAVE_PATH   =   './logs/'\n",
        "BATCH_SIZE  =   100\n",
        "LEARN_RATE  =   0.001\n",
        "EPOCHS      =   100\n",
        "\n",
        "INPUT_H     = 64\n",
        "INPUT_W     = 64\n",
        "INPUT_CH    = 1\n",
        "\n",
        "LATENT_DIM  = 128\n",
        "NUM_GENERATE_SAMPLES    =   100\n",
        "DATA_VAR = 0.037"
      ],
      "metadata": {
        "id": "JNTR5mGY1JNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VQ VAE - tinyImage Dataset"
      ],
      "metadata": {
        "id": "DkScfeTZ1l9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH   =   './datasets/tiny_imagenet/'\n",
        "\n",
        "FILE_EXTN   =   '.JPEG'\n",
        "SAVE_PATH   =   './logs/'\n",
        "BATCH_SIZE  =   128\n",
        "LEARN_RATE  =   0.0002\n",
        "EPOCHS      =   100\n",
        "\n",
        "INPUT_H     = 64\n",
        "INPUT_W     = 64\n",
        "INPUT_CH    = 3\n",
        "\n",
        "EMBED_H     = 4\n",
        "EMBED_W     = 4\n",
        "EMBED_CH    = 128\n",
        "\n",
        "NUM_EMBEDDINGS = 128\n",
        "BETA            = 0.25\n",
        "NUM_GENERATE_SAMPLES    =   100\n",
        "DATA_VAR = 0.0765\n",
        "\n",
        "# FIT GMM ON LATENT SPACE - AFTER PCA\n",
        "N_GMM_COMPONENTS = 32\n",
        "GMM_FIT_NUM_SAMPLES = 10000\n",
        "GMM_GEN_NUM_SAMPLES = 100\n",
        "GMM_NUM_PCA_FEATURES = 256\n",
        "\n",
        "# FIT VANILLA VAE ON LATENT SPACE\n",
        "VAE_FIT_NUM_SAMPLES =   100 #samples in a batch\n",
        "VAE_FIT_LEARN_RATE  =   0.001\n",
        "VAE_FIT_EPOCHS      =   50\n",
        "\n",
        "VAE_FIT_LATENT_DIM  = 32\n",
        "VAE_GEN_NUM_SAMPLES    =   100\n",
        "VAE_FIT_DATA_VAR = 0.077"
      ],
      "metadata": {
        "id": "Zz640Bz91Nz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataLoader Class and Utilities Definitions"
      ],
      "metadata": {
        "id": "cq6Hd1sy2olg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self,img_folder, extn='.jpg'):\n",
        "        self.img_folder=img_folder   \n",
        "        self.extn = extn\n",
        "        self.img_list = [name for name in os.listdir(self.img_folder) if name.endswith(self.extn)]\n",
        "        return\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.img_list)\n",
        "    \n",
        "    def __getitem__(self,index):     \n",
        "        image=read_image(self.img_folder+'/'+self.img_list[index])          \n",
        "        image=image.float()                \n",
        "        image=image/255.0        \n",
        "        return image\n",
        "\n",
        "def getDataloader(data_path, batch_size, extn):\n",
        "    print('[INFO] DATA_PATH={}, BATCH_SIZE={}'.format(data_path,batch_size))\n",
        "    imgDataset = ImageDataset(data_path,extn)    \n",
        "    print('[INFO] Found data set with {} samples'.format(len(imgDataset)))\n",
        "    dl = DataLoader(imgDataset, batch_size,\n",
        "                    shuffle=True,worker_init_fn=seed_worker,generator=g)\n",
        "    return dl\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(cfg.DATA_PATH)\n",
        "    data = getDataloader(cfg.DATA_PATH, cfg.BATCH_SIZE, cfg.FILE_EXTN)\n",
        "    for image_batch in data:        \n",
        "        print(image_batch.size())\n",
        "        print(torch.var(image_batch))"
      ],
      "metadata": {
        "id": "Dsrw3xpO2wqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets as DS\n",
        "def save_image_to_file(epoch,image_tensor, save_path,ref_str=None):\n",
        "    print(image_tensor.size())\n",
        "    if ref_str is not None:\n",
        "        filestr = save_path + ref_str +'SAMPLE_IMGS_E'+ str(epoch)  + '.jpg'\n",
        "    else:\n",
        "        filestr = save_path + 'SAMPLE_IMGS_E'+ str(epoch)  + '.jpg'\n",
        "    save_image(image_tensor,filestr,nrow = 10) \n",
        "    return\n",
        "\n",
        "def return_random_batch_from_dir(img_folder, file_extn, num_samples):\n",
        "    img_list = [name for name in os.listdir(img_folder) if name.endswith(file_extn)]\n",
        "    samples=[]\n",
        "    if(len(img_list)>0):\n",
        "        \n",
        "        sample_names = random.sample(img_list, num_samples)\n",
        "        for name in sample_names:\n",
        "            img = read_image(img_folder+'/'+name).float()\n",
        "            img = img/255.0\n",
        "            samples.append((img.unsqueeze(0)))\n",
        "        samples = torch.cat(samples)\n",
        "        print(samples.size())\n",
        "    return samples"
      ],
      "metadata": {
        "id": "DEZkOGPu3LLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vanilla VAE Class Definitions"
      ],
      "metadata": {
        "id": "Q9jY8ES01q6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### FOR USE WITH CELEBA/DSPRITES DATASET ###\n",
        "class Encoder(Module):\n",
        "    def __init__(self, in_channels, latent_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.latent_dim = latent_dim\n",
        "        self.in_channels = in_channels\n",
        "        self.output_dim = self.latent_dim\n",
        "\n",
        "        self.conv1 = Conv2d(in_channels=self.in_channels, out_channels=32,kernel_size=3, stride=2, padding=1)\n",
        "        self.bnorm1 = BatchNorm2d(32)\n",
        "        self.relu1 = LeakyReLU()        \n",
        "        \n",
        "        self.conv2 = Conv2d(in_channels=32, out_channels=64,kernel_size=3, stride=2, padding=1)\n",
        "        self.bnorm2 = BatchNorm2d(64)\n",
        "        self.relu2 = LeakyReLU()\n",
        "\n",
        "        self.conv3 = Conv2d(in_channels=64, out_channels=128,kernel_size=3, stride=2, padding=1)\n",
        "        self.bnorm3 = BatchNorm2d(128)\n",
        "        self.relu3 = LeakyReLU()\n",
        "\n",
        "        self.conv4 = Conv2d(in_channels=128, out_channels=256,kernel_size=3, stride=2, padding=1)\n",
        "        self.bnorm4 = BatchNorm2d(256)\n",
        "        self.relu4 = LeakyReLU()\n",
        "        \n",
        "        #self.fc1 = Linear(in_features=256*4, out_features=256)\t        \n",
        "\n",
        "        self.fcMu = Linear(in_features=256*4*4, out_features=self.output_dim)\n",
        "        self.fcCov = Linear(in_features=256*4*4, out_features=self.output_dim)\n",
        "\n",
        "        self.relu5 = ReLU()\n",
        "\n",
        "        return\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.bnorm1(self.relu1((self.conv1(x))))\n",
        "        x = self.bnorm2(self.relu2((self.conv2(x))))\n",
        "        x = self.bnorm3(self.relu3((self.conv3(x))))\n",
        "        x = self.bnorm4(self.relu4((self.conv4(x))))\n",
        "        \n",
        "        x = flatten(x, start_dim=1)                \n",
        "        out_mu = self.fcMu(x)\n",
        "        out_cov = self.relu5(self.fcCov(x))\n",
        "\n",
        "        #out_cov = self.fcCov(x) # log variance\n",
        "        out = torch.cat((out_mu, out_cov),dim=1)\n",
        "        return out\n",
        "\n",
        "class Decoder(Module):\n",
        "    def __init__(self, latent_dim, output_dim, output_channels):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.input_dim = latent_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.output_channels = output_channels\n",
        "\n",
        "        self.fc1 = Linear(self.input_dim, out_features=256*4*4)\t        \n",
        "        \n",
        "        self.convT1 = ConvTranspose2d(in_channels=256, out_channels=128,kernel_size=3, stride=2, padding=1,output_padding=1)\n",
        "        self.bnorm1 = BatchNorm2d(128)\n",
        "        self.relu1 = LeakyReLU()\n",
        "\n",
        "        self.convT2 = ConvTranspose2d(in_channels=128, out_channels=64,kernel_size=3, stride=2, padding=1,output_padding=1)\n",
        "        self.bnorm2 = BatchNorm2d(64)\n",
        "        self.relu2 = LeakyReLU()\n",
        "\n",
        "        self.convT3 = ConvTranspose2d(in_channels=64, out_channels=32,kernel_size=3, stride=2, padding=1,output_padding=1)\n",
        "        self.bnorm3 = BatchNorm2d(32)\n",
        "        self.relu3 = LeakyReLU()\n",
        "\n",
        "        self.convT31 = ConvTranspose2d(in_channels=32, out_channels=32,kernel_size=3, stride=2, padding=1,output_padding=1)\n",
        "        self.bnorm31 = BatchNorm2d(32)\n",
        "        self.relu31 = LeakyReLU()\n",
        "        \n",
        "        self.convT4 = ConvTranspose2d(in_channels=32, out_channels=self.output_channels,kernel_size=3, padding=1)\n",
        "        #self.bnorm4 = BatchNorm2d(3)\n",
        "        self.tanh = Tanh()            \n",
        "\n",
        "        return\n",
        "    \n",
        "    def forward(self,x):\n",
        "        \n",
        "        x = self.fc1(x)        \n",
        "        \n",
        "        x = torch.reshape(x,(-1,256,4,4))\n",
        "        \n",
        "        x = self.bnorm1(self.relu1(self.convT1(x)))\n",
        "        \n",
        "        x = self.bnorm2(self.relu2(self.convT2(x)))\n",
        "        \n",
        "        x = self.bnorm3(self.relu3(self.convT3(x)))\n",
        "        \n",
        "        x = self.bnorm31(self.relu31(self.convT31(x)))\n",
        "        \n",
        "        out = self.tanh(self.convT4(x))     \n",
        "         \n",
        "        return 0.5 + (0.5*out)\n",
        "\n",
        "# Define VAE class\n",
        "class Varn_AE(Module):\n",
        "    def __init__(self, input_dim, latent_dim, in_channels, var_norm):\n",
        "        super().__init__()\n",
        "\n",
        "        self.eps = 0.000001\n",
        "        self.input_dim = input_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.in_channels = in_channels\n",
        "        self.var_norm = var_norm\n",
        "\n",
        "        self.mse_loss = torch.nn.MSELoss(reduction='sum')        \n",
        "        self.kld_loss = self.kld_gaussian\n",
        "\n",
        "        self.mse_loss_value = 0.0\n",
        "        self.kl_loss_value = 0.0\n",
        "\n",
        "        self.mu = torch.zeros(self.latent_dim)\n",
        "        self.dCov = torch.zeros(self.latent_dim)\n",
        "        \n",
        "        self.encoder = Encoder(self.in_channels,self.latent_dim)\n",
        "        self.decoder = Decoder(self.latent_dim, self.input_dim, self.in_channels)\n",
        "        \n",
        "        self.encoder.float()\n",
        "        self.decoder.float()\n",
        "\n",
        "        print()\n",
        "        print('-'*59)\n",
        "        print('ENCODER MODEL')\n",
        "        print('-'*59)\n",
        "        print(self.encoder)\n",
        "\n",
        "        print('-'*59)\n",
        "        print('DECODER MODEL')\n",
        "        print('-'*59)\n",
        "        print(self.decoder)\n",
        "        print()\n",
        "        return\n",
        "\n",
        "    def forward(self,x):        \n",
        "        \n",
        "        ## ENCODE ##\n",
        "        x = self.encoder(x)\n",
        "\n",
        "        ## SAMPLE ##\n",
        "        self.mu = x[:,:self.latent_dim]\n",
        "        self.dCov = x[:,self.latent_dim:] + self.eps\n",
        "        \n",
        "        eps = torch.randn_like(self.dCov)                \n",
        "        z = self.mu + (eps*self.dCov)\n",
        "\n",
        "        ## DECODE ##\n",
        "        x_hat=self.decoder(z)\n",
        "\n",
        "        return x_hat\n",
        "    \n",
        "    def kld_gaussian(self):   \n",
        "        mu_sq = torch.square(self.mu)\n",
        "        loss_j = (1.0 + torch.log(self.dCov)) - mu_sq - self.dCov\n",
        "        kld_loss = -1*0.5*torch.sum(loss_j)\n",
        "        return kld_loss\n",
        "\n",
        "    def criterion(self, x, x_hat):            \n",
        "                \n",
        "        x_hat = torch.reshape(x_hat, (x.size(0),-1))        \n",
        "        \n",
        "        x = torch.reshape(x,(x.size(0),-1,))        \n",
        "        \n",
        "        self.mse_loss_value = self.mse_loss(x,x_hat)\n",
        "\n",
        "        #self.mse_loss_value = self.mse_loss_value/(2*self.var_norm)\n",
        "        self.kl_loss_value = self.kld_loss()        \n",
        "        \n",
        "        return self.kl_loss_value + self.mse_loss_value, self.mse_loss_value, self.kl_loss_value        \n",
        "    \n",
        "    def sample(self,num_samples=100, curr_device=\"cpu\"):\n",
        "        self.decoder.eval()\n",
        "        z = torch.randn((num_samples,self.latent_dim),device=curr_device)\n",
        "        samples = self.decoder(z)\n",
        "        #print(samples.size())\n",
        "        self.decoder.train()\n",
        "        return samples\n"
      ],
      "metadata": {
        "id": "K7hHO14p1-vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### FOR USE WITH VQ-VAE ###\n",
        "class Encoder(Module):\n",
        "    def __init__(self, in_channels, latent_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.latent_dim = latent_dim\n",
        "        self.in_channels = in_channels\n",
        "        self.output_dim = self.latent_dim\n",
        "\n",
        "        self.conv1 = Conv2d(in_channels=self.in_channels, out_channels=32,kernel_size=3, stride=2, padding=1)\n",
        "        self.bnorm1 = BatchNorm2d(32)\n",
        "        self.relu1 = LeakyReLU()        \n",
        "        \n",
        "        self.conv2 = Conv2d(in_channels=32, out_channels=64,kernel_size=3, stride=2, padding=1)\n",
        "        self.bnorm2 = BatchNorm2d(64)\n",
        "        self.relu2 = LeakyReLU()\n",
        "\n",
        "        self.conv3 = Conv2d(in_channels=64, out_channels=128,kernel_size=3, stride=2, padding=1)\n",
        "        self.bnorm3 = BatchNorm2d(128)\n",
        "        self.relu3 = LeakyReLU()\n",
        "\n",
        "        self.conv4 = Conv2d(in_channels=128, out_channels=256,kernel_size=3, stride=2, padding=1)\n",
        "        self.bnorm4 = BatchNorm2d(256)\n",
        "        self.relu4 = LeakyReLU()\n",
        "                \n",
        "        self.fcMu = Linear(in_features=256, out_features=self.output_dim)\n",
        "        self.fcCov = Linear(in_features=256, out_features=self.output_dim)\n",
        "        self.relu5 = ReLU()\n",
        "\n",
        "        return\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.bnorm1(self.relu1((self.conv1(x))))\n",
        "        x = self.bnorm2(self.relu2((self.conv2(x))))\n",
        "        x = self.bnorm3(self.relu3((self.conv3(x))))\n",
        "        x = self.bnorm4(self.relu4((self.conv4(x))))\n",
        "        \n",
        "        x = flatten(x, start_dim=1)                \n",
        "        out_mu = self.fcMu(x)\n",
        "        out_cov = self.relu5(self.fcCov(x))\n",
        "\n",
        "        #out_cov = self.fcCov(x) # log variance\n",
        "        out = torch.cat((out_mu, out_cov),dim=1)\n",
        "        return out\n",
        "\n",
        "class Decoder(Module):\n",
        "    def __init__(self, latent_dim, output_dim, output_channels):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.input_dim = latent_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.output_channels = output_channels\n",
        "\n",
        "        self.fc1 = Linear(self.input_dim, out_features=16*4*4)\t        \n",
        "        \n",
        "        self.convT1 = ConvTranspose2d(in_channels=16, out_channels=8,kernel_size=3, stride=1, padding=1)\n",
        "        self.bnorm1 = BatchNorm2d(8)\n",
        "        self.relu1 = LeakyReLU()\n",
        "\n",
        "        self.convT2 = ConvTranspose2d(in_channels=8, out_channels=4,kernel_size=3, stride=1, padding=1)\n",
        "        self.bnorm2 = BatchNorm2d(4)\n",
        "        self.relu2 = LeakyReLU()\n",
        "\n",
        "        self.convT3 = ConvTranspose2d(in_channels=4, out_channels=4,kernel_size=3, stride=1, padding=1)\n",
        "        self.bnorm3 = BatchNorm2d(4)\n",
        "        self.relu3 = LeakyReLU()\n",
        "\n",
        "        self.convT31 = ConvTranspose2d(in_channels=4, out_channels=4,kernel_size=3, stride=1, padding=1)\n",
        "        self.bnorm31 = BatchNorm2d(4)\n",
        "        self.relu31 = LeakyReLU()\n",
        "        \n",
        "        self.convT4 = ConvTranspose2d(in_channels=4, out_channels=self.output_channels,kernel_size=3, padding=1)\n",
        "        #self.bnorm4 = BatchNorm2d(3)\n",
        "        self.tanh = Tanh()            \n",
        "\n",
        "        return\n",
        "    \n",
        "    def forward(self,x):\n",
        "        \n",
        "        x = self.fc1(x)        \n",
        "        \n",
        "        x = torch.reshape(x,(-1,16,4,4))\n",
        "        \n",
        "        x = self.bnorm1(self.relu1(self.convT1(x)))\n",
        "        \n",
        "        x = self.bnorm2(self.relu2(self.convT2(x)))\n",
        "        \n",
        "        x = self.bnorm3(self.relu3(self.convT3(x)))\n",
        "        \n",
        "        x = self.bnorm31(self.relu31(self.convT31(x)))\n",
        "        \n",
        "        out = self.tanh(self.convT4(x))     \n",
        "         \n",
        "        return 0.5 + (0.5*out)\n",
        "\n",
        "# Define VAE class\n",
        "class Varn_AE(Module):\n",
        "    def __init__(self, input_dim, latent_dim, in_channels, var_norm):\n",
        "        super().__init__()\n",
        "\n",
        "        self.eps = 0.000001\n",
        "        self.input_dim = input_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.in_channels = in_channels\n",
        "        self.var_norm = var_norm\n",
        "\n",
        "        self.mse_loss = torch.nn.MSELoss(reduction='sum')        \n",
        "        self.kld_loss = self.kld_gaussian\n",
        "\n",
        "        self.mse_loss_value = 0.0\n",
        "        self.kl_loss_value = 0.0\n",
        "\n",
        "        self.mu = torch.zeros(self.latent_dim)\n",
        "        self.dCov = torch.zeros(self.latent_dim)\n",
        "        \n",
        "        self.encoder = Encoder(self.in_channels,self.latent_dim)\n",
        "        self.decoder = Decoder(self.latent_dim, self.input_dim, self.in_channels)\n",
        "        \n",
        "        self.encoder.float()\n",
        "        self.decoder.float()\n",
        "\n",
        "        print()\n",
        "        print('-'*59)\n",
        "        print('ENCODER MODEL')\n",
        "        print('-'*59)\n",
        "        print(self.encoder)\n",
        "\n",
        "        print('-'*59)\n",
        "        print('DECODER MODEL')\n",
        "        print('-'*59)\n",
        "        print(self.decoder)\n",
        "        print()\n",
        "        return\n",
        "\n",
        "    def forward(self,x):        \n",
        "        \n",
        "        ## ENCODE ##\n",
        "        x = self.encoder(x)\n",
        "\n",
        "        ## SAMPLE ##\n",
        "        self.mu = x[:,:self.latent_dim]\n",
        "        self.dCov = x[:,self.latent_dim:] + self.eps\n",
        "        \n",
        "        eps = torch.randn_like(self.dCov)                \n",
        "        z = self.mu + (eps*self.dCov)\n",
        "\n",
        "        ## DECODE ##\n",
        "        x_hat=self.decoder(z)\n",
        "\n",
        "        return x_hat\n",
        "    \n",
        "    def kld_gaussian(self):   \n",
        "        mu_sq = torch.square(self.mu)\n",
        "        loss_j = (1.0 + torch.log(self.dCov)) - mu_sq - self.dCov\n",
        "        kld_loss = -1*0.5*torch.sum(loss_j)\n",
        "        return kld_loss\n",
        "\n",
        "    def criterion(self, x, x_hat):            \n",
        "                \n",
        "        x_hat = torch.reshape(x_hat, (x.size(0),-1))        \n",
        "        \n",
        "        x = torch.reshape(x,(x.size(0),-1,))        \n",
        "        \n",
        "        self.mse_loss_value = self.mse_loss(x,x_hat)\n",
        "\n",
        "        self.kl_loss_value = self.kld_loss()        \n",
        "        \n",
        "        return self.kl_loss_value + self.mse_loss_value, self.mse_loss_value, self.kl_loss_value        \n",
        "    \n",
        "    def sample(self,num_samples=100, curr_device=\"cpu\"):\n",
        "        self.decoder.eval()\n",
        "        z = torch.randn((num_samples,self.latent_dim),device=curr_device)\n",
        "        samples = self.decoder(z)      \n",
        "        self.decoder.train()\n",
        "        return samples\n"
      ],
      "metadata": {
        "id": "XaITJ9au2EDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VQ-VAE Class Definitions"
      ],
      "metadata": {
        "id": "ZZq3R58i3jE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        self.conv1 = Conv2d(in_channels=self.in_channels, out_channels=32,kernel_size=3, stride=2, padding=1)\n",
        "        self.bnorm1 = BatchNorm2d(32)\n",
        "        self.relu1 = LeakyReLU()        \n",
        "        \n",
        "        self.conv2 = Conv2d(in_channels=32, out_channels=64,kernel_size=3, stride=2, padding=1)\n",
        "        self.bnorm2 = BatchNorm2d(64)\n",
        "        self.relu2 = LeakyReLU()\n",
        "\n",
        "        self.conv3 = Conv2d(in_channels=64, out_channels=128,kernel_size=3, stride=2, padding=1)\n",
        "        self.bnorm3 = BatchNorm2d(128)\n",
        "        self.relu3 = LeakyReLU()\n",
        "\n",
        "        self.conv4 = Conv2d(in_channels=128, out_channels=self.out_channels,kernel_size=3, stride=2, padding=1)\n",
        "        self.bnorm4 = BatchNorm2d(self.out_channels)\n",
        "        self.relu4 = LeakyReLU()               \n",
        "\n",
        "        return\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.bnorm1(self.relu1((self.conv1(x))))\n",
        "        \n",
        "        x = self.bnorm2(self.relu2((self.conv2(x))))\n",
        "        \n",
        "        x = self.bnorm3(self.relu3((self.conv3(x))))\n",
        "        \n",
        "        out = self.bnorm4(self.relu4((self.conv4(x))))\n",
        "                \n",
        "        return out\n",
        "\n",
        "class Decoder(Module):\n",
        "    def __init__(self, latent_dim, output_dim, output_channels):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.input_dim = latent_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.output_channels = output_channels         \n",
        "        \n",
        "        self.convT1 = ConvTranspose2d(in_channels=128, out_channels=64,kernel_size=3, stride=2, padding=1,output_padding=1)\n",
        "        self.bnorm1 = BatchNorm2d(64)\n",
        "        self.relu1 = LeakyReLU()\n",
        "\n",
        "        self.convT2 = ConvTranspose2d(in_channels=64, out_channels=32,kernel_size=3, stride=2, padding=1,output_padding=1)\n",
        "        self.bnorm2 = BatchNorm2d(32)\n",
        "        self.relu2 = LeakyReLU()\n",
        "\n",
        "        self.convT3 = ConvTranspose2d(in_channels=32, out_channels=16,kernel_size=3, stride=2, padding=1,output_padding=1)\n",
        "        self.bnorm3 = BatchNorm2d(16)\n",
        "        self.relu3 = LeakyReLU()\n",
        "\n",
        "        self.convT31 = ConvTranspose2d(in_channels=16, out_channels=16,kernel_size=3, stride=2, padding=1,output_padding=1)\n",
        "        self.bnorm31 = BatchNorm2d(16)\n",
        "        self.relu31 = LeakyReLU()\n",
        "        \n",
        "        self.convT4 = ConvTranspose2d(in_channels=16, out_channels=self.output_channels,kernel_size=3, padding=1)\n",
        "        #self.bnorm4 = BatchNorm2d(3)\n",
        "        self.tanh = Tanh()            \n",
        "\n",
        "        return\n",
        "    \n",
        "    def forward(self,x):\n",
        "                \n",
        "        \n",
        "        x = torch.reshape(x,(-1,128,4,4))\n",
        "        \n",
        "        x = self.bnorm1(self.relu1(self.convT1(x)))\n",
        "        \n",
        "        x = self.bnorm2(self.relu2(self.convT2(x)))\n",
        "        \n",
        "        x = self.bnorm3(self.relu3(self.convT3(x)))\n",
        "        \n",
        "        x = self.bnorm31(self.relu31(self.convT31(x)))\n",
        "        \n",
        "        out = self.tanh(self.convT4(x))     \n",
        "         \n",
        "        return 0.5 + (0.5*out)\n",
        "\n",
        "class VQ(Module):\n",
        "    def __init__(self, embedding_dim, num_embeddings, beta):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_embeddings = num_embeddings        \n",
        "\n",
        "        self.mse_loss = MSELoss(reduction='sum')                \n",
        "                \n",
        "        self.embedding_loss_value = 0.0\n",
        "        self.commitment_loss_value = 0.0            \n",
        "        \n",
        "        self.dictionary = Embedding(self.num_embeddings, self.embedding_dim)\n",
        "        self.dictionary.weight.data.uniform_(-1/self.num_embeddings, 1/self.num_embeddings)\n",
        "        \n",
        "        self.beta = beta\n",
        "        return        \n",
        "\n",
        "    def findNearest(self,x):\n",
        "        # x is BC X HW => BC x EMBED_DIM\n",
        "        # xref is NUM_EMBED X EMBED_DIM\n",
        "\n",
        "        xref = self.dictionary.weight\n",
        "        x_norm2 = torch.linalg.norm(x, dim=1,keepdim=True)**2 #[BC x 1]\n",
        "        x_norm2 = x_norm2.expand(-1,self.num_embeddings) #[BC x NUM_EMBED]\n",
        "\n",
        "        xref_norm2 = torch.linalg.norm(xref, dim=1,keepdim=True)**2 #[NUM_EMBED x 1]\n",
        "        xref_norm2 = xref_norm2.expand(-1, x.size(0)) #[NUM_EMBED x BC]\n",
        "        xref_norm2 = torch.transpose(xref_norm2, 0, 1) #[BC X NUM_EMBED]        \n",
        "        \n",
        "        dist = x_norm2 + xref_norm2 - 2*torch.matmul(x, torch.transpose(xref, 0 , 1))        \n",
        "\n",
        "        nearest_idxs =  torch.argmin(dist, dim=1).unsqueeze(1)\n",
        "        \n",
        "        return nearest_idxs\n",
        "\n",
        "    def forward(self,z_e):        \n",
        "        \n",
        "        z_e = torch.reshape(z_e,(z_e.size(0)*z_e.size(1),z_e.size(2)*z_e.size(3))) #BC x HW => BCxEMBED_DIM\n",
        "        ## Find nearest embeddings      \n",
        "        nearest_idxs = self.findNearest(z_e)\n",
        "        \n",
        "        ## DECODE ##\n",
        "        z_q = self.dictionary.weight[nearest_idxs,:].squeeze()   \n",
        "        #print(z_e.size(), z_q.size())\n",
        "        self.embedding_loss_value = self.mse_loss(z_e.detach(),z_q)\n",
        "        self.commitment_loss_value = self.mse_loss(z_e,z_q.detach())\n",
        "\n",
        "        self.vq_loss = self.embedding_loss_value + self.beta*self.commitment_loss_value\n",
        "        \n",
        "        z_q = z_e + (z_q - z_e).detach()\n",
        "\n",
        "        return z_q, self.vq_loss    \n",
        "\n",
        "class VQ_Varn_AE(Module):\n",
        "    def __init__(self, input_dim, embedding_dim, in_channels, embed_channels, var_norm, num_embeddings, beta):\n",
        "        super().__init__()\n",
        "                \n",
        "        self.input_dim = input_dim        \n",
        "        self.in_channels = in_channels\n",
        "        self.embed_channels = embed_channels\n",
        "        self.var_norm = var_norm\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.beta = beta\n",
        "\n",
        "        self.mse_loss = BCELoss(reduction='sum')                \n",
        "        \n",
        "        self.mse_loss_value = 0.0        \n",
        "                \n",
        "        #print(self.dictionary.weight.size())\n",
        "        self.encoder = Encoder(self.in_channels,self.embed_channels)\n",
        "        self.vq = VQ(self.embedding_dim, self.num_embeddings, self.beta)\n",
        "        self.decoder = Decoder(self.embedding_dim, self.input_dim, self.in_channels)\n",
        "        \n",
        "        self.encoder.float()\n",
        "        self.vq.float()\n",
        "        self.decoder.float()\n",
        "\n",
        "        print()\n",
        "        print('-'*59)\n",
        "        print('ENCODER MODEL')\n",
        "        print('-'*59)\n",
        "        print(self.encoder)\n",
        "\n",
        "        print('-'*59)\n",
        "        print('VQ LAYER')\n",
        "        print('-'*59)\n",
        "        print(self.vq)\n",
        "\n",
        "        print('-'*59)\n",
        "        print('DECODER MODEL')\n",
        "        print('-'*59)\n",
        "        print(self.decoder)\n",
        "        print()\n",
        "\n",
        "        return\n",
        "    \n",
        "    def forward(self,x):        \n",
        "               \n",
        "        z_e = self.encoder(x)            \n",
        "        \n",
        "        ## Find nearest embeddings              \n",
        "        z_q, vq_loss = self.vq(z_e)      \n",
        "        \n",
        "        ## DECODE ##                               \n",
        "        x_hat = self.decoder(z_q)        \n",
        "        \n",
        "        return x_hat, vq_loss\n",
        "    \n",
        "    def criterion(self, x, x_hat, vq_loss):            \n",
        "                \n",
        "        x_hat = torch.reshape(x_hat, (x.size(0),-1))                \n",
        "        x = torch.reshape(x,(x.size(0),-1,))        \n",
        "        \n",
        "        self.mse_loss_value = self.mse_loss(x_hat,x)        \n",
        "        \n",
        "        total_loss = self.mse_loss_value + vq_loss\n",
        "\n",
        "        return total_loss, self.mse_loss_value\n",
        "    \n",
        "    def sample(self, x_in=None, num_samples=100, curr_device=\"cpu\"):\n",
        "        self.decoder.eval()\n",
        "        self.vq.eval()\n",
        "        self.encoder.eval()\n",
        "\n",
        "        if(x_in == None):\n",
        "            return None\n",
        "        else:\n",
        "            z_e = self.encoder(x_in)              \n",
        "            z_q, _ = self.vq(z_e)                \n",
        "            samples = self.decoder(z_q)                    \n",
        "        \n",
        "        self.decoder.train()\n",
        "        self.vq.train()\n",
        "        self.encoder.train()\n",
        "        \n",
        "        return samples\n",
        "\n"
      ],
      "metadata": {
        "id": "z7foEBvf3nlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Runner - VAE (Question 1)"
      ],
      "metadata": {
        "id": "1sAaxRy537-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "import config_celeba as cfg\n",
        "\n",
        "import utils as util\n",
        "\n",
        "from datasets import getDataloader\n",
        "from vae_2_CNN import Varn_AE\n",
        "#from torch.distributions.multivariate_normal import MultivariateNormal\n",
        "#########################LOGGER#########################\n",
        "import sys\n",
        "\n",
        "class Logger(object):\n",
        "    def __init__(self, filename=\"Default.log\"):\n",
        "        self.terminal = sys.stdout\n",
        "        self.log = open(filename, \"a\")\n",
        "\n",
        "    def write(self, message):\n",
        "        self.terminal.write(message)\n",
        "        self.log.write(message)\n",
        "    \n",
        "    def flush(self):\n",
        "        pass\n",
        "\n",
        "sys.stdout = Logger(cfg.SAVE_PATH + 'expt_1_celeba.txt')\n",
        "#########################################################3\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "#########################################################3\n",
        "def train_VAE():\n",
        "    print('-' * 59)\n",
        "    torch.cuda.empty_cache()\n",
        "    vae_model = Varn_AE(cfg.INPUT_H*cfg.INPUT_W, cfg.LATENT_DIM, cfg.INPUT_CH, cfg.DATA_VAR)\n",
        "    vae_model.to(device)\n",
        "    vae_model.train()\n",
        "    \n",
        "    optimizer = torch.optim.Adam(vae_model.parameters(), lr=cfg.LEARN_RATE)\n",
        "\n",
        "    data = getDataloader(cfg.DATA_PATH,cfg.BATCH_SIZE, cfg.FILE_EXTN)\n",
        "    N = len(data)*cfg.BATCH_SIZE\n",
        "    print('-' * 59)\n",
        "    print(\"Starting Training of model\")\n",
        "    epoch_times = []\n",
        "\n",
        "    for epoch in range(1,cfg.EPOCHS+1):        \n",
        "        start_time = time.process_time()        \n",
        "        total_loss = 0.0\n",
        "        mse_loss = 0.0\n",
        "        kl_loss = 0.0\n",
        "        counter = 0        \n",
        "        for image_batch in data:\n",
        "            if(image_batch.size(1)>cfg.INPUT_CH):\n",
        "                image_batch = image_batch[:,0,:,:].unsqueeze(1)\n",
        "            counter += 1            \n",
        "            optimizer.zero_grad()           \n",
        "            #print(image_batch.size())\n",
        "            #print(aaa)\n",
        "            x_hat = vae_model.forward(image_batch.to(device)) \n",
        "                    \n",
        "            loss,m,k = vae_model.criterion(image_batch.to(device), x_hat)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            mse_loss += m.item()\n",
        "            kl_loss += k.item()\n",
        "            if counter%500 == 0:                \n",
        "                print(\"Epoch {}......Step: {}/{}....... Loss={:12.5} (MSE Loss = {:12.5}, KL Loss= {:12.5})\"\n",
        "                .format(epoch, counter, len(data), total_loss/N,mse_loss/N,kl_loss/N))\n",
        "        \n",
        "        current_time = time.process_time()\n",
        "        print(\"Epoch {}/{} Done, Loss = {:12.5} (MSE Loss = {:12.5}, KL Loss= {:12.5})\"\n",
        "        .format(epoch, cfg.EPOCHS, total_loss/N,mse_loss/N,kl_loss/N))\n",
        "\n",
        "        print(\"Total Time Elapsed={:12.5} seconds\".format(str(current_time-start_time)))\n",
        "\n",
        "        if((mse_loss/N) < 500):            \n",
        "            samples = vae_model.sample(cfg.NUM_GENERATE_SAMPLES,device)\n",
        "            util.save_image_to_file(epoch,samples, cfg.SAVE_PATH)\n",
        "        if((mse_loss/N) < 500):      \n",
        "            torch.save(vae_model, cfg.SAVE_PATH + 'MODEL_E' + str(epoch) + datetime.date.today().strftime(\"%B %d, %Y\") + '.pth')\n",
        "        \n",
        "        epoch_times.append(current_time-start_time)\n",
        "        print('-' * 59)\n",
        "\n",
        "    print(\"Total Training Time={:12.5} seconds\".format(str(sum(epoch_times))))\n",
        "    return vae_model\n",
        "\n",
        "def compute_mv_normal(x,mu=None,var=1):\n",
        "    d = x.shape[1]\n",
        "    n = x.shape[0]\n",
        "    if mu is None:\n",
        "        mu = np.zeros((d,))\n",
        "\n",
        "    K = -0.5*d*np.log(2*np.pi*var)\n",
        "    logp = np.zeros((n,))\n",
        "    for i in range(0,n):\n",
        "        exponent = np.linalg.norm(x[i,:]-mu)**2\n",
        "        exponent =-1.0*exponent/(2*var)\n",
        "        logp[i] = exponent + K\n",
        "\n",
        "    return logp\n",
        "\n",
        "def estimate_marginal_prob(vae_model, num_input_samples):\n",
        "    vae_model.to(device)\n",
        "    vae_model.eval()\n",
        "    data = getDataloader(cfg.DATA_PATH,num_input_samples, cfg.FILE_EXTN)\n",
        "\n",
        "    for image_batch in data:\n",
        "        #logprob(vae_model, image_batch.to(device))\n",
        "        z = vae_model.encoder(image_batch.to(device))\n",
        "        \n",
        "        n = int(z.size(1)/2)\n",
        "        mu = z[:,:n].unsqueeze(1).expand(-1,cfg.NUM_LATENT_SAMPLES_PER_IP,-1)\n",
        "        dCov = z[:,n:].unsqueeze(1).expand(-1,cfg.NUM_LATENT_SAMPLES_PER_IP,-1)\n",
        "        \n",
        "        eps = torch.randn_like(dCov)                \n",
        "        z_1 = mu + (eps*dCov)\n",
        "        \n",
        "        eps = torch.randn_like(dCov)                \n",
        "        z_2 = mu + (eps*dCov)\n",
        "        \n",
        "        marginal_p = np.zeros((num_input_samples,))\n",
        "        for i in range(0,num_input_samples):\n",
        "            x = image_batch[i,:,:,:].detach().numpy()\n",
        "\n",
        "            z1_sample = z_1[i,:,:].squeeze().cpu().detach().numpy()\n",
        "            q_z = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(z1_sample)\n",
        "\n",
        "            x_hat = vae_model.decoder(z_2[i,:,:].squeeze()).cpu().detach().numpy()         \n",
        "            \n",
        "            z2_sample = z_2[i,:,:].squeeze().cpu().detach().numpy()\n",
        "            \n",
        "            # all probabilities are in log\n",
        "            q_prob = q_z.score_samples(z2_sample)\n",
        "            p_prob = compute_mv_normal(z2_sample,None,0.584)            \n",
        "            x_hat = np.reshape(x_hat,(-1,cfg.INPUT_H*cfg.INPUT_W*cfg.INPUT_CH))\n",
        "            x = np.reshape(np.expand_dims(x, axis=0),(-1,cfg.INPUT_H*cfg.INPUT_W*cfg.INPUT_CH))\n",
        "            \n",
        "            p_x_z_prob = compute_mv_normal(x_hat,x,0.5)\n",
        "            \n",
        "            logp = q_prob - (p_prob + p_x_z_prob)\n",
        "            \n",
        "            p_inv = np.mean(np.exp(logp))\n",
        "            marginal_p[i] = 1/p_inv\n",
        "            \n",
        "        break\n",
        "    return marginal_p\n",
        "    "
      ],
      "metadata": {
        "id": "6GvIRZux4C6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Runner - VQ VAE (Question 3)"
      ],
      "metadata": {
        "id": "wNUz7pPd44q9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import config_tiny_imagenet as cfg\n",
        "\n",
        "import utils as util\n",
        "\n",
        "from datasets import getDataloader\n",
        "from vq_vae_2_CNN import VQ_Varn_AE\n",
        "from vae_3_CNN import Varn_AE\n",
        "\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.decomposition import PCA\n",
        "#########################LOGGER#########################\n",
        "import sys\n",
        "\n",
        "class Logger(object):\n",
        "    def __init__(self, filename=\"Default.log\"):\n",
        "        self.terminal = sys.stdout\n",
        "        self.log = open(filename, \"a\")\n",
        "\n",
        "    def write(self, message):\n",
        "        self.terminal.write(message)\n",
        "        self.log.write(message)\n",
        "    \n",
        "    def flush(self):\n",
        "        pass\n",
        "\n",
        "sys.stdout = Logger(cfg.SAVE_PATH + 'expt_3_tinyimagenet.txt')\n",
        "#########################################################3\n",
        "\n",
        "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "#########################################################3\n",
        "def train_VAE():\n",
        "    print('-' * 59)\n",
        "    torch.cuda.empty_cache()\n",
        "    vae_model = VQ_Varn_AE(cfg.INPUT_H*cfg.INPUT_W, cfg.EMBED_H*cfg.EMBED_W, cfg.INPUT_CH, cfg.EMBED_CH ,cfg.DATA_VAR, cfg.NUM_EMBEDDINGS, cfg.BETA)\n",
        "    #print(aaa)\n",
        "    vae_model.to(device)    \n",
        "    vae_model.train()\n",
        "    \n",
        "    optimizer = torch.optim.Adam(vae_model.parameters(), lr=cfg.LEARN_RATE)\n",
        "\n",
        "    data = getDataloader(cfg.DATA_PATH,cfg.BATCH_SIZE, cfg.FILE_EXTN)\n",
        "    N = len(data)*cfg.BATCH_SIZE\n",
        "    print('-' * 59)\n",
        "    print(\"Starting Training of model\")\n",
        "    epoch_times = []\n",
        "\n",
        "    for epoch in range(1,cfg.EPOCHS+1):        \n",
        "        start_time = time.process_time()        \n",
        "        total_loss = 0.0\n",
        "        mse_loss = 0.0\n",
        "        vq = 0.0\n",
        "        \n",
        "        counter = 0        \n",
        "        for image_batch in data:\n",
        "            #print(image_batch.size())\n",
        "            #print(torch.var(image_batch))            \n",
        "            counter += 1            \n",
        "            optimizer.zero_grad()\n",
        "            x_hat, vq_loss,_ = vae_model.forward(image_batch.to(device))                     \n",
        "            loss, m = vae_model.criterion(image_batch.to(device), x_hat, vq_loss)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            mse_loss += m.item()\n",
        "            vq += vq_loss.item()            \n",
        "            \n",
        "            if counter%500 == 0:                \n",
        "                print(\"Epoch {}......Step: {}/{}....... Loss={:12.5} (BCE Loss = {:12.5}, EMB + COMT Loss= {:12.5})\"\n",
        "                .format(epoch, counter, len(data), total_loss/N,mse_loss/N,vq/N))\n",
        "        \n",
        "        current_time = time.process_time()\n",
        "        print(\"Epoch {}/{} Done, Loss = {:12.5} (BCE Loss = {:12.5}, EMB + COMT Loss= {:12.5})\"\n",
        "        .format(epoch, cfg.EPOCHS, total_loss/N,mse_loss/N,vq/N))\n",
        "\n",
        "        print(\"Total Time Elapsed={:12.5} seconds\".format(str(current_time-start_time)))\n",
        "\n",
        "        if((mse_loss/N) < 10000):\n",
        "            samples = util.return_random_batch_from_dir(cfg.DATA_PATH, cfg.FILE_EXTN, cfg.NUM_GENERATE_SAMPLES)\n",
        "            r_samples = vae_model.sample(samples.to(device), cfg.NUM_GENERATE_SAMPLES,device)\n",
        "            util.save_image_to_file(epoch,r_samples, cfg.SAVE_PATH,'RECON')\n",
        "            util.save_image_to_file(epoch, samples, cfg.SAVE_PATH, 'ORIG')\n",
        "        if((mse_loss/N) < 10000):\n",
        "            torch.save(vae_model, cfg.SAVE_PATH + 'MODEL_E' + str(epoch) + datetime.date.today().strftime(\"%B %d, %Y\") + '.pth')\n",
        "        \n",
        "        epoch_times.append(current_time-start_time)\n",
        "        print('-' * 59)\n",
        "\n",
        "    print(\"Total Training Time={:12.5} seconds\".format(str(sum(epoch_times))))\n",
        "    return vae_model\n",
        "\n",
        "def fit_vae_training_data(vq_vae_model):\n",
        "    vq_vae_model.eval()\n",
        "    data = getDataloader(cfg.DATA_PATH,cfg.VAE_FIT_NUM_SAMPLES, cfg.FILE_EXTN)    \n",
        "\n",
        "    vae_model = Varn_AE(cfg.EMBED_H*cfg.EMBED_W, cfg.VAE_FIT_LATENT_DIM, cfg.EMBED_CH, cfg.VAE_FIT_DATA_VAR)\n",
        "    vae_model.to(device)\n",
        "    vae_model.train()\n",
        "    optimizer = torch.optim.Adam(vae_model.parameters(), lr=cfg.VAE_FIT_LEARN_RATE)\n",
        "\n",
        "    N = len(data)*cfg.VAE_FIT_NUM_SAMPLES\n",
        "    print('-' * 59)\n",
        "    print(\"Starting Training of VAE model on latent space\")\n",
        "    epoch_times = []\n",
        "\n",
        "    for epoch in range(1,cfg.VAE_FIT_EPOCHS+1):        \n",
        "        start_time = time.process_time()        \n",
        "        total_loss = 0.0\n",
        "        mse_loss = 0.0\n",
        "        kl_loss = 0.0\n",
        "        counter = 0        \n",
        "        for image_batch in data:\n",
        "            \n",
        "            z_q,_ = vq_vae_model.vq(vq_vae_model.encoder(image_batch.to(device)))\n",
        "            z_q = torch.reshape(z_q,(-1,cfg.EMBED_CH,cfg.EMBED_H,cfg.EMBED_W))\n",
        "            \n",
        "            counter += 1            \n",
        "            optimizer.zero_grad()\n",
        "            x_hat = vae_model.forward(z_q) \n",
        "            #print(x_hat.size(),z_q.size())\n",
        "            loss,m,k = vae_model.criterion(z_q.to(device), x_hat)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            mse_loss += m.item()\n",
        "            kl_loss += k.item()\n",
        "            if counter%500 == 0:                \n",
        "                print(\"Epoch {}......Step: {}/{}....... Loss={:12.5} (MSE Loss = {:12.5}, KL Loss= {:12.5})\"\n",
        "                .format(epoch, counter, len(data), total_loss/N,mse_loss/N,kl_loss/N))\n",
        "        \n",
        "        current_time = time.process_time()\n",
        "        print(\"Epoch {}/{} Done, Loss = {:12.5} (MSE Loss = {:12.5}, KL Loss= {:12.5})\"\n",
        "        .format(epoch, cfg.EPOCHS, total_loss/N,mse_loss/N,kl_loss/N))\n",
        "\n",
        "        print(\"Total Time Elapsed={:12.5} seconds\".format(str(current_time-start_time)))\n",
        "\n",
        "        if((mse_loss/N) < 500):            \n",
        "            samples = vae_model.sample(cfg.NUM_GENERATE_SAMPLES,device)\n",
        "            util.save_image_to_file(epoch,samples, cfg.SAVE_PATH)\n",
        "        if((mse_loss/N) < 500):      \n",
        "            torch.save(vae_model, cfg.SAVE_PATH + 'MODEL_E' + str(epoch) + datetime.date.today().strftime(\"%B %d, %Y\") + '.pth')\n",
        "        \n",
        "        epoch_times.append(current_time-start_time)\n",
        "        print('-' * 59)\n",
        "\n",
        "    print(\"Total Training Time={:12.5} seconds\".format(str(sum(epoch_times))))\n",
        "\n",
        "    return vae_model\n",
        "\n",
        "def fit_gmm_training_data(vq_vae_model):\n",
        "    vq_vae_model.eval()\n",
        "\n",
        "    gmm = GaussianMixture(n_components=cfg.N_GMM_COMPONENTS,covariance_type='diag', \n",
        "                            max_iter=500,verbose=3,tol=0.0001, \n",
        "                            verbose_interval=10,\n",
        "                            random_state=42)\n",
        "\n",
        "    pca = PCA(n_components=cfg.GMM_NUM_PCA_FEATURES, whiten=True)\n",
        "\n",
        "    data = getDataloader(cfg.DATA_PATH,cfg.GMM_FIT_NUM_SAMPLES, cfg.FILE_EXTN)    \n",
        "\n",
        "    print('-' * 59)\n",
        "    print(\"Generate latent space samples from training data\")\n",
        "    \n",
        "    for image_batch in data:\n",
        "        #_,_,idxs = vq_vae_model.vq(vq_vae_model.encoder(image_batch.to(device)))\n",
        "        z_q = vq_vae_model.encoder(image_batch.to(device))\n",
        "        #print(z_q.size()) \n",
        "        z_q = torch.reshape(z_q,(-1,cfg.EMBED_H*cfg.EMBED_W*cfg.EMBED_CH))\n",
        "        #print(idxs.size()) \n",
        "        #print(aaa)             \n",
        "        print(\"Reduce data dimensionality of latent samples:\")\n",
        "        reduced_z_q = pca.fit_transform(z_q.cpu().detach().numpy())  \n",
        "        #reduced_z_q=idxs\n",
        "        print(\"Fit GMM on latent samples of this batch:\")\n",
        "        gmm.fit(reduced_z_q)\n",
        "        break\n",
        "\n",
        "    return gmm, pca\n",
        "\n",
        "def generate_samples_from_gmm_fit(gmm,pca,vq_vae_model, num_samples):\n",
        "    vq_vae_model.eval()\n",
        "    s_reduced_data = gmm.sample(num_samples)[0]\n",
        "    latent_samples = torch.from_numpy(pca.inverse_transform(s_reduced_data))\n",
        "\n",
        "    latent_samples = torch.reshape(latent_samples,(num_samples*cfg.EMBED_CH,cfg.EMBED_H*cfg.EMBED_W))\n",
        "    #latent_samples = torch.reshape(latent_samples,(num_samples,cfg.EMBED_CH,cfg.EMBED_H,cfg.EMBED_W))\n",
        "    print(latent_samples.size())\n",
        "    \n",
        "    r_samples = vq_vae_model.decoder(latent_samples.float().to(device))\n",
        "    #r_samples = vq_vae_model.decoder(vq_vae_model.vq(latent_samples.float().to(device))[0])\n",
        "    util.save_image_to_file(99,r_samples, cfg.SAVE_PATH,'GMM_FIT_')\n",
        "    return\n",
        "\n",
        "def generate_samples_from_vae_fit(vq_vae_model, vae_model, num_samples):\n",
        "    vq_vae_model.eval()\n",
        "    vae_model.eval()\n",
        "    latent_samples = vae_model.sample(num_samples, device)\n",
        "    print(latent_samples.size())\n",
        "\n",
        "    latent_samples = torch.reshape(latent_samples,(num_samples*cfg.EMBED_CH,cfg.EMBED_H*cfg.EMBED_W))\n",
        "    print(latent_samples.size())\n",
        "    \n",
        "    r_samples = vq_vae_model.decoder(latent_samples.float().to(device))\n",
        "    util.save_image_to_file(99,r_samples, cfg.SAVE_PATH,'VAE_FIT_')\n",
        "    return\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    vq_vae_model = train_VAE()\n",
        "    #vq_vae_model = torch.load('./logs/MODEL_E100September 25, 2022.pth')\n",
        "    \n",
        "    #gmm, pca = fit_gmm_training_data(vq_vae_model)\n",
        "    #generate_samples_from_gmm_fit(gmm,pca,vq_vae_model,cfg.GMM_GEN_NUM_SAMPLES)\n",
        "\n",
        "    #vae_model =fit_vae_training_data(vq_vae_model)\n",
        "    #generate_samples_from_vae_fit(vq_vae_model, vae_model, cfg.VAE_GEN_NUM_SAMPLES)"
      ],
      "metadata": {
        "id": "mnv2HOwT0ugA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yJV_S2i90vCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generative Adversarial Networks (GAN)\n"
      ],
      "metadata": {
        "id": "X2YsG9xt08E2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DCGAN\n"
      ],
      "metadata": {
        "id": "9vFe8Env1HRV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UX_PQ-qr3inn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we will implement DCGAN on SVHN dataset. The dataset is available at http://ufldl.stanford.edu/housenumbers/. The dataset is a collection of 32x32 color images of house numbers. The dataset is split into 3 parts: train, test and extra. We will use the train and test set for training and testing respectively. The extra set is not used in this tutorial.\n",
        "model_name = \"dcgan1\"\n",
        "#check model saving path is there\n",
        "# Imports\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms as transformations\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import PIL.Image as Image\n",
        "import torchvision.models as models\n",
        "#numpy\n",
        "import numpy as np\n",
        "import tqdm\n",
        "from ignite.metrics.gan import FID\n",
        "# Hyperparameters and constants: for dataset and training\n",
        "\n",
        "# Hyperparameters etc.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "LEARNING_RATE = 2e-4  # could also use two lrs, one for gen and one for disc\n",
        "BATCH_SIZE = 3200\n",
        "IMAGE_SIZE = 64\n",
        "CHANNELS_IMG = 3\n",
        "NOISE_DIM = 100\n",
        "NUM_EPOCHS = 5\n",
        "FEATURES_DISC = 64\n",
        "FEATURES_GEN = 64\n",
        "# preparing Dataset\n",
        "### we will use SVHN dataset for this example\n",
        "### we will combine the train, test and extra datasets to make a bigger dataset\n",
        "\n",
        "transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(IMAGE_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "#get the dataset\n",
        "\n",
        "dataset = dataset = datasets.ImageFolder(root=os.path.join(os.getcwd(), \"bitemojis_dataset\"), transform=transform)\n",
        "loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "#print the total number of images in the dataset\n",
        "print(len(dataset))\n",
        "# print the shape of the images\n",
        "print(dataset[0][0].shape)\n",
        "# print the label of the image\n",
        "print(dataset[0][1])\n",
        "# Model\n",
        "## generator\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, channels_noise, channels_img, features_g):\n",
        "        super(Generator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "           \n",
        "            self.generator_block_architecture(channels_noise, features_g * 16, 4, 1, 0),  # img: 4x4\n",
        "            self.generator_block_architecture(features_g * 16, features_g * 8, 4, 2, 1),  # img: 8x8\n",
        "            self.generator_block_architecture(features_g * 8, features_g * 4, 4, 2, 1),  # img: 16x16\n",
        "            self.generator_block_architecture(features_g * 4, features_g * 2, 4, 2, 1),  # img: 32x32\n",
        "            nn.ConvTranspose2d(\n",
        "                features_g * 2, channels_img, kernel_size=4, stride=2, padding=1\n",
        "            ),\n",
        "     \n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def generator_block_architecture(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                stride,\n",
        "                padding,\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "## discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, channels_img, features_d):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.disc = nn.Sequential(\n",
        "\n",
        "            nn.Conv2d(\n",
        "                channels_img, features_d, kernel_size=4, stride=2, padding=1\n",
        "            ),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # _block(in_channels, out_channels, kernel_size, stride, padding)\n",
        "            self.dicriminator_block_architecture(features_d, features_d * 2, 4, 2, 1),\n",
        "            self.dicriminator_block_architecture(features_d * 2, features_d * 4, 4, 2, 1),\n",
        "            self.dicriminator_block_architecture(features_d * 4, features_d * 8, 4, 2, 1),\n",
        "       \n",
        "            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def dicriminator_block_architecture(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                stride,\n",
        "                padding,\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.disc(x)\n",
        "\n",
        "--------\n",
        "# Initialization : Model , Loss , Optimizer, data loader\n",
        "### model\n",
        "def initialize_weights(model):\n",
        "    # Initializes weights according to the DCGAN paper\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
        "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "### data loader\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "gen = Generator(NOISE_DIM, CHANNELS_IMG, FEATURES_GEN).to(device)\n",
        "disc = Discriminator(CHANNELS_IMG, FEATURES_DISC).to(device)\n",
        "initialize_weights(gen)\n",
        "initialize_weights(disc)\n",
        "### optimizer, loss\n",
        "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "criterion = nn.BCELoss()\n",
        "### tensorboard\n",
        "\n",
        "fixed_noise = torch.randn(100, NOISE_DIM, 1, 1).to(device)\n",
        "#plot loss of generator and critic\n",
        "writer_loss = SummaryWriter(f\"runs/\"+model_name+\"/loss\")\n",
        "writer_real = SummaryWriter(f\"logs/\"+model_name+\"/real\")\n",
        "writer_fake = SummaryWriter(f\"logs/\"+model_name+\"/fake\")\n",
        "----\n",
        "### initialize FID wrapper\n",
        "fid_score = FID()\n",
        "#interpolate function to resize images to 299,299,3  which is the input size of inception network\n",
        "def interpolate(batch):\n",
        "    arr = []\n",
        "    for img in batch:\n",
        "        pil_img = transformations.ToPILImage()(img)\n",
        "        resized_img = pil_img.resize((299,299), Image.BILINEAR)\n",
        "        arr.append(transformations.ToTensor()(resized_img))\n",
        "    return torch.stack(arr)\n",
        "# Training\n",
        "gen.train()\n",
        "disc.train()\n",
        "step = 0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "    \n",
        "    #we will track the total loss of the generator and critic for each epoch over the entire dataset\n",
        "    #initialize the total loss of the generator and critic for each epoch to 0\n",
        "    total_loss_gen = 0\n",
        "    total_loss_disc = 0\n",
        "    #move these to device\n",
        "    \n",
        "    \n",
        "    # Target labels not needed! <3 unsupervised\n",
        "    for batch_idx, (real, _) in enumerate(dataloader):\n",
        "        batch_step = 0\n",
        "        real = real.to(device)\n",
        "        noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1, 1).to(device)\n",
        "        fake = gen(noise)\n",
        "\n",
        "        ### Train Discriminator\n",
        "        disc_real = disc(real).reshape(-1)\n",
        "        loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
        "        disc_fake = disc(fake.detach()).reshape(-1)\n",
        "        loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
        "        loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
        "        disc.zero_grad()\n",
        "        loss_disc.backward()\n",
        "        opt_disc.step()\n",
        "\n",
        "        ### Train Generator:\n",
        "        output = disc(fake).reshape(-1)\n",
        "        loss_gen = criterion(output, torch.ones_like(output))\n",
        "        gen.zero_grad()\n",
        "        loss_gen.backward()\n",
        "        opt_gen.step()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            total_loss_gen += loss_gen.item()\n",
        "            total_loss_disc += loss_disc.item()\n",
        "            \n",
        "        \n",
        "\n",
        "        # Print losses occasionally and print to tensorboard\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(\n",
        "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(dataloader)} \\\n",
        "                  Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\"\n",
        "            )\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                               \n",
        "                 #BATCH LOSS---\n",
        "        \n",
        "                #write loss to tensorboard\n",
        "                writer_loss.add_scalar(\"Generator loss Batch\", loss_gen, global_step=batch_step)\n",
        "                writer_loss.add_scalar(\"Discriminator loss Batch\", loss_disc, global_step=batch_step)         \n",
        "                \n",
        "                #FID--\n",
        "                #calculate FID score of this batch\n",
        "                #update the fid_score with real and fake images\n",
        "                real_images_fid = interpolate(real)\n",
        "                fake_images_fid = interpolate(fake)\n",
        "                fid_score.update((real_images_fid, fake_images_fid))\n",
        "                computed_fid_score = fid_score.compute()\n",
        "                print(\"FID score: \", computed_fid_score)\n",
        "                writer_loss.add_scalar(\"FID Score DCGAN\", computed_fid_score, global_step=batch_step)\n",
        "                #reset the fid score\n",
        "                fid_score.reset()\n",
        "                ##FID--\n",
        "                \n",
        "                batch_step += 1 \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "\n",
        "    with torch.no_grad():\n",
        "        fake = gen(fixed_noise)\n",
        "        # take out upto 100 examples\n",
        "        img_grid_real = torchvision.utils.make_grid(\n",
        "            real[:100], normalize=True\n",
        "        )\n",
        "        img_grid_fake = torchvision.utils.make_grid(\n",
        "            fake[:100], normalize=True\n",
        "        )\n",
        "\n",
        "        writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
        "        writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
        "        \n",
        "        \n",
        "        #AVERAGE LOSS---\n",
        "\n",
        "        #get average loss of generator and critic for each epoch\n",
        "        avg_loss_gen = total_loss_gen / len(loader)\n",
        "        avg_loss_disc= total_loss_disc / len(loader)\n",
        "        #write loss to tensorboard\n",
        "        writer_loss.add_scalar(\"Generator loss Epoch\", avg_loss_gen, global_step=batch_step)\n",
        "        writer_loss.add_scalar(\"Discriminator loss Epoch\", avg_loss_disc, global_step=batch_step)\n",
        "        \n",
        "        #AVERAGE LOSS----\n",
        "        \n",
        "        #we will plot the gradient of disc output with respect to the input image\n",
        "        #get the gradient of the disc output with respect to the input image\n",
        "        gradient = torch.autograd.grad(\n",
        "        inputs=real,\n",
        "        outputs=disc_real,\n",
        "        grad_outputs=torch.ones_like(disc_real),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        )[0]\n",
        "        #flatten the gradient\n",
        "        gradient = gradient.view(gradient.shape[0], -1)\n",
        "        #get the norm of the gradient\n",
        "        gradient_norm = gradient.norm(2, dim=1)\n",
        "        #write gradient norm to tensorboard\n",
        "        writer_loss.add_scalar(\"Gradient norm Disc Real DCGAN\", gradient_norm.mean(), global_step=step)\n",
        "        \n",
        "        #----------------\n",
        "        #we will plot the gradient of critic output with respect to the input image\n",
        "        #get the gradient of the critic output with respect to the input image\n",
        "        gradient = torch.autograd.grad(\n",
        "        inputs=fake,\n",
        "        outputs=disc_fake,\n",
        "        grad_outputs=torch.ones_like(disc_fake),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        )[0]\n",
        "        #flatten the gradient\n",
        "        gradient = gradient.view(gradient.shape[0], -1)\n",
        "        #get the norm of the gradient\n",
        "        gradient_norm = gradient.norm(2, dim=1)\n",
        "        #write gradient norm to tensorboard\n",
        "        writer_loss.add_scalar(\"Gradient norm Disc Fake DCGAN\", gradient_norm.mean(), global_step=step)\n",
        "        \n",
        "        #----------------\n",
        "        #we will plot the gradient of genrator output with respect to the input \n",
        "        #we will plot the gradient of genrator output with respect to the input \n",
        "        #get the gradient of the generator output with respect to the input noise\n",
        "        gradient = torch.autograd.grad(\n",
        "        inputs=noise,\n",
        "        outputs=output,\n",
        "        grad_outputs=torch.ones_like(output),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        )[0]\n",
        "        #flatten the gradient\n",
        "        gradient = gradient.view(gradient.shape[0], -1)\n",
        "        #get the norm of the gradient\n",
        "        gradient_norm = gradient.norm(2, dim=1)\n",
        "        #write gradient norm to tensorboard\n",
        "        writer_loss.add_scalar(\"Gradient norm Generator DCGAN\", gradient_norm.mean(), global_step=step)\n",
        "        \n",
        "        #----------------\n",
        "        \n",
        "        #get the gradient of the disc for the parameters weights of first layer\n",
        "        #we will write the norm of the gardient of weights of the first layer of the disc\n",
        "        for name, param in critic.named_parameters():\n",
        "            if name == \"disc.0.weight\":\n",
        "                writer_loss.add_scalar(\"Disc Gradient w.r.t 1st layer DCGAN\", param.grad.norm(), global_step=step)\n",
        "            #also plot the norm of gradient of 2nd layer\n",
        "            elif name == \"disc.2.0.weight\":\n",
        "                writer_loss.add_scalar(\"Disc Gradient w.r.t 2nd layer DCGAN\", param.grad.norm(), global_step=step)\n",
        "                \n",
        "                \n",
        "       \n",
        "\n",
        "    step += 1\n",
        "    \n",
        "    #save the trained model\n",
        "        #check if trained_model folder exists\n",
        "    if not os.path.exists(\"trained_models\"):\n",
        "        os.mkdir(\"trained_models\")\n",
        "    \n",
        "    #now trained_model folder exists\n",
        "    if not os.path.exists(\"trained_models/\"+model_name):\n",
        "        os.mkdir(\"trained_models/\"+model_name)\n",
        "    #check if \"trained_models/\"+model_name     \n",
        "    torch.save(gen.state_dict(), \"trained_models/\"+model_name+\"/gen.pth\")\n",
        "    torch.save(critic.state_dict(), \"trained_models/\"+model_name+\"/disc.pth\")\n",
        "    \n",
        "\n",
        "#save the tensorboard\n",
        "writer_real.close()\n",
        "writer_fake.close()\n",
        "writer_loss.close()"
      ],
      "metadata": {
        "id": "LunG49r216xP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conditional WGAN"
      ],
      "metadata": {
        "id": "6rRRSLK6qiVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we will train a conditional wgan on svhn dataset\n",
        "# we will use the gradient penalty to stabilize the training\n",
        "\n",
        "\n",
        "model_name = \"c_wgan7\"\n",
        "#check model saving path is there\n",
        "# imports\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms as transformations\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import PIL.Image as Image\n",
        "import torchvision.models as models\n",
        "#numpy\n",
        "import numpy as np\n",
        "import tqdm\n",
        "from ignite.metrics.gan import FID\n",
        "# hyperparameters for the dataset and the MOdel\n",
        "# Hyperparameters etc.\n",
        "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
        "LEARNING_RATE = 1e-4\n",
        "BATCH_SIZE = 64000\n",
        "IMAGE_SIZE = 64\n",
        "CHANNELS_IMG = 3\n",
        "NUM_CLASSES = 10\n",
        "GEN_EMBEDDING = 100\n",
        "Z_DIM = 100\n",
        "NUM_EPOCHS = 50\n",
        "FEATURES_CRITIC = 16\n",
        "FEATURES_GEN = 16\n",
        "CRITIC_ITERATIONS = 5\n",
        "LAMBDA_GP = 10\n",
        "\n",
        "# prepare the dataset\n",
        "### we will use SVHN dataset for this example\n",
        "### we will combine the train, test and extra datasets to make a bigger dataset\n",
        "transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(IMAGE_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]),\n",
        "    ]\n",
        ")\n",
        "#get the dataset\n",
        "#train part of svhn\n",
        "train_dataset = datasets.SVHN(root=\"dataset_svhm/\", split='train', transform=transforms, download=True)\n",
        "#test part of svhn\n",
        "test_dataset = datasets.SVHN(root=\"dataset_svhm/\", split='test', transform=transforms, download=True)\n",
        "#extra part of svhn\n",
        "extra_dataset = datasets.SVHN(root=\"dataset_svhm/\", split='extra', transform=transforms, download=True)\n",
        "#concatenate the train, test and extra dataset\n",
        "dataset = torch.utils.data.ConcatDataset([train_dataset, test_dataset, extra_dataset])\n",
        "loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "#print the total number of images in the dataset\n",
        "print(len(dataset))\n",
        "# print the shape of the images\n",
        "print(dataset[0][0].shape)\n",
        "# print the label of the image\n",
        "print(dataset[0][1])\n",
        "# Model\n",
        "\n",
        "## generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, channels_noise, channels_img, features_g , num_classes, img_size, embed_size):\n",
        "        super(Generator, self).__init__()\n",
        "        self.img_size = img_size\n",
        "        self.net = nn.Sequential(\n",
        "            # Input: N x channels_noise x 1 x 1\n",
        "            self.block_architecture_generator(channels_noise+embed_size, features_g * 16, 4, 1, 0),  # img: 4x4\n",
        "            self.block_architecture_generator(features_g * 16, features_g * 8, 4, 2, 1),  # img: 8x8\n",
        "            self.block_architecture_generator(features_g * 8, features_g * 4, 4, 2, 1),  # img: 16x16\n",
        "            self.block_architecture_generator(features_g * 4, features_g * 2, 4, 2, 1),  # img: 32x32\n",
        "            nn.ConvTranspose2d(\n",
        "                features_g * 2, channels_img, kernel_size=4, stride=2, padding=1\n",
        "            ),\n",
        "            # Output: N x channels_img x 64 x 64\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "        \n",
        "        self.embed = nn.Embedding(num_classes, embed_size)\n",
        "\n",
        "    def block_architecture_generator(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(\n",
        "                in_channels, out_channels, kernel_size, stride, padding, bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, labels):\n",
        "        embedding = self.embed(labels).unsqueeze(2).unsqueeze(3)\n",
        "        x = torch.cat([x, embedding], 1)\n",
        "        return self.net(x)\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, channels_img, features_d, num_classes, img_size):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.img_size = img_size\n",
        "        self.disc = nn.Sequential(\n",
        "            \n",
        "            nn.Conv2d(channels_img+1, features_d, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # _block(in_channels, out_channels, kernel_size, stride, padding)\n",
        "            self.block_architecture_critic(features_d, features_d * 2, 4, 2, 1),\n",
        "            self.block_architecture_critic(features_d * 2, features_d * 4, 4, 2, 1),\n",
        "            self.block_architecture_critic(features_d * 4, features_d * 8, 4, 2, 1),\n",
        "            # After all _block img output is 4x4 (Conv2d below makes into 1x1)\n",
        "            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0),\n",
        "        )\n",
        "        \n",
        "        #embedding for conditionning\n",
        "        self.embed = nn.Embedding(num_classes, img_size * img_size)\n",
        "\n",
        "    def block_architecture_critic(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels, out_channels, kernel_size, stride, padding, bias=False,\n",
        "            ),\n",
        "            nn.InstanceNorm2d(out_channels, affine=True),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, labels):\n",
        "        embedding = self.embed(labels).view(labels.shape[0], 1, self.img_size, self.img_size)\n",
        "        x = torch.cat([x, embedding], dim=1)\n",
        "        return self.disc(x)\n",
        "--------------------------------\n",
        "# model initialization\n",
        "\n",
        "def initialize_weights(model):\n",
        "    # Initializes weights according to the DCGAN paper\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
        "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "\n",
        "\n",
        "# initialize gen and critic\n",
        "gen = Generator(Z_DIM, CHANNELS_IMG, FEATURES_GEN, NUM_CLASSES, IMAGE_SIZE, GEN_EMBEDDING).to(device)\n",
        "critic = Discriminator(CHANNELS_IMG, FEATURES_CRITIC, NUM_CLASSES, IMAGE_SIZE).to(device)\n",
        "initialize_weights(gen)\n",
        "initialize_weights(critic)\n",
        "### initialize optimizer\n",
        "# initializate optimizer\n",
        "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))\n",
        "opt_critic = optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))\n",
        "### inintialize tensorboard\n",
        "# for tensorboard plotting\n",
        "fixed_noise = torch.randn(100, Z_DIM, 1, 1).to(device)\n",
        "#fixed labels for tensorboard plotting\n",
        "# we will have fixed labels of integers between 0 and 9 for the 10 classes\n",
        "fixed_labels = torch.randint(0, 10, (100,)).to(device)\n",
        "\n",
        "#plot loss of generator and critic\n",
        "writer_loss = SummaryWriter(f\"runs/\"+model_name+\"/loss\")\n",
        "writer_real = SummaryWriter(f\"logs/\"+model_name+\"/real\")\n",
        "writer_fake = SummaryWriter(f\"logs/\"+model_name+\"/fake\")\n",
        "#\n",
        "--------------------------------\n",
        "### initialize FID wrapper\n",
        "fid_score = FID()\n",
        "#interpolate function to resize images to 299,299,3  which is the input size of inception network\n",
        "def interpolate(batch):\n",
        "    arr = []\n",
        "    for img in batch:\n",
        "        pil_img = transformations.ToPILImage()(img)\n",
        "        resized_img = pil_img.resize((299,299), Image.BILINEAR)\n",
        "        arr.append(transformations.ToTensor()(resized_img))\n",
        "    return torch.stack(arr)\n",
        "#TEST FID \n",
        "\n",
        "# y_pred, y = torch.rand(100, 3, 64, 64), torch.rand(100, 3, 64, 64)\n",
        "# y_pred = interpolate(y_pred)\n",
        "# y = interpolate(y)\n",
        "# # m = FID()\n",
        "# fid_score.update((y_pred, y))\n",
        "\n",
        "# print('ignite batch FID', fid_score.compute())  # 8.98434072559458e-05\n",
        "# #reset the fid score\n",
        "# fid_score.reset()\n",
        "## start training\n",
        "\n",
        "\n",
        "gen.train()\n",
        "critic.train()\n",
        "#### gradient penalty function for WGAN-GP\n",
        "def gradient_penalty(critic,labels, real, fake, device=\"cpu\"):\n",
        "    BATCH_SIZE, C, H, W = real.shape\n",
        "    alpha = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
        "    interpolated_images = real * alpha + fake * (1 - alpha)\n",
        "\n",
        "    # Calculate critic scores\n",
        "    mixed_scores = critic(interpolated_images, labels)\n",
        "\n",
        "    # Take the gradient of the scores with respect to the images\n",
        "    gradient = torch.autograd.grad(\n",
        "        inputs=interpolated_images,\n",
        "        outputs=mixed_scores,\n",
        "        grad_outputs=torch.ones_like(mixed_scores),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "    )[0]\n",
        "    gradient = gradient.view(gradient.shape[0], -1)\n",
        "    gradient_norm = gradient.norm(2, dim=1)\n",
        "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
        "    return gradient_penalty\n",
        "step = 0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    #we will track the total loss of the generator and critic for each epoch over the entire dataset\n",
        "    #initialize the total loss of the generator and critic for each epoch to 0\n",
        "    total_loss_gen = 0\n",
        "    total_loss_critic = 0\n",
        "    \n",
        "    #move these to device\n",
        "\n",
        "    #have no gradient for these losse\n",
        "    # total_loss_gen = torch.no_grad()\n",
        "    # total_loss_critic = torch.no_grad()\n",
        "    \n",
        "    # Target labels \n",
        "    for batch_idx, (real, labels) in enumerate(loader):\n",
        "        #send labels to device\n",
        "        labels = labels.to(device)\n",
        "        batch_step = 0\n",
        "        real = real.to(device)\n",
        "        cur_batch_size = real.shape[0]\n",
        "        \n",
        "        \n",
        "\n",
        "        # Train Critic: \n",
        "        for _ in range(CRITIC_ITERATIONS):\n",
        "            noise = torch.randn(cur_batch_size, Z_DIM, 1, 1).to(device)\n",
        "            if len(noise) != len(labels):\n",
        "                noise = noise[:len(labels)]\n",
        "            fake = gen(noise, labels)\n",
        "            critic_real = critic(real, labels).reshape(-1)\n",
        "            critic_fake = critic(fake, labels).reshape(-1)\n",
        "            gp = gradient_penalty(critic, real, fake, device=device)\n",
        "            loss_critic = (\n",
        "                -(torch.mean(critic_real) - torch.mean(critic_fake)) + LAMBDA_GP * gp\n",
        "            )\n",
        "            \n",
        "            \n",
        "            critic.zero_grad()\n",
        "            loss_critic.backward(retain_graph=True)\n",
        "            opt_critic.step()\n",
        "            \n",
        "        #trained critic\n",
        "        \n",
        "\n",
        "        # Train Generator: \n",
        "        gen_fake = critic(fake, labels).reshape(-1)\n",
        "        loss_gen = -torch.mean(gen_fake)\n",
        "        gen.zero_grad()\n",
        "        loss_gen.backward()\n",
        "        opt_gen.step()\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        #update the total loss of the generator and critic for each batch in the epoch\n",
        "        #add just the value no gradients\n",
        "        #have no gradient for these losse\n",
        "        #add just the value no gradientsfrom the loss_gen tensor\n",
        "      \n",
        "        with torch.no_grad():\n",
        "            total_loss_gen += loss_gen.item()\n",
        "            total_loss_critic += loss_critic.item()\n",
        "        \n",
        "        # Print losses occasionally and print to tensorboard in a batch\n",
        "        if batch_idx % 10 == 0 and batch_idx > 0:\n",
        "            print(\n",
        "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(loader)} \\\n",
        "                  Loss D: {loss_critic:.4f}, loss G: {loss_gen:.4f}\"\n",
        "            )\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                \n",
        "                #BATCH LOSS-----\n",
        "                \n",
        "                #write gen_loss and critic_loss to tensorboard\n",
        "                writer_loss.add_scalar(\"Generator loss Batch\", loss_gen, global_step=step)\n",
        "                writer_loss.add_scalar(\"Critic loss Batch\", loss_critic, global_step=step)\n",
        "                \n",
        "                #BATCH LOSS-------\n",
        "\n",
        "                \n",
        "                #FID--\n",
        "                #calculate FID score of this batch\n",
        "                #update the fid_score with real and fake images\n",
        "                real_images_fid = interpolate(real)\n",
        "                fake_images_fid = interpolate(fake)\n",
        "                fid_score.update((real_images_fid, fake_images_fid))\n",
        "                computed_fid_score = fid_score.compute()\n",
        "                print(\"FID score: \", computed_fid_score)\n",
        "                writer_loss.add_scalar(\"FID Score WGAN\", computed_fid_score, global_step=batch_step)\n",
        "                #reset the fid score\n",
        "                fid_score.reset()\n",
        "                ##FID--\n",
        "                \n",
        "                batch_step += 1\n",
        "            \n",
        "            \n",
        "        \n",
        "        \n",
        "\n",
        "        # Print losses occasionally and print to tensorboard per epoch\n",
        "        # if batch_idx % 100 == 0 and batch_idx > 0:\n",
        "        \n",
        "            ## PRINT for few epochs %10\n",
        "            # print(\n",
        "            #     f\"Epoch [{epoch}/{NUM_EPOCHS}]  \\\n",
        "            #         Loss D: {loss_critic:.4f}, loss G: {loss_gen:.4f}\"\n",
        "            # )\n",
        "            \n",
        "    #calculate the Frchet Inception Distance (FID) to evaluate the performance of the generator\n",
        "    \n",
        "\n",
        "    with torch.no_grad():\n",
        "        fake = gen(fixed_noise, fixed_labels)\n",
        "        # take out (up to) 32 examples\n",
        "        img_grid_real = torchvision.utils.make_grid(real[:100], normalize=True)\n",
        "        img_grid_fake = torchvision.utils.make_grid(fake[:100], normalize=True)\n",
        "\n",
        "        writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
        "        writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
        "        \n",
        "        \n",
        "                        \n",
        "        #AVERAGE LOSS---\n",
        "\n",
        "        #get average loss of generator and critic for each epoch\n",
        "        avg_loss_gen = total_loss_gen / len(loader)\n",
        "        avg_loss_critic = total_loss_critic / len(loader)\n",
        "        #write loss to tensorboard\n",
        "        writer_loss.add_scalar(\"Generator loss Epoch\", avg_loss_gen, global_step=batch_step)\n",
        "        writer_loss.add_scalar(\"Critic loss Epoch\", avg_loss_critic, global_step=batch_step)\n",
        "        \n",
        "        #AVERAGE LOSS----\n",
        "        \n",
        "        \n",
        "        \n",
        "        #we will plot the gradient of critic output with respect to the input image\n",
        "        #get the gradient of the critic output with respect to the input image\n",
        "        gradient = torch.autograd.grad(\n",
        "        inputs=real,\n",
        "        outputs=critic_real,\n",
        "        grad_outputs=torch.ones_like(critic_real),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        )[0]\n",
        "        #flatten the gradient\n",
        "        gradient = gradient.view(gradient.shape[0], -1)\n",
        "        #get the norm of the gradient\n",
        "        gradient_norm = gradient.norm(2, dim=1)\n",
        "        #write gradient norm to tensorboard\n",
        "        writer_loss.add_scalar(\"Gradient norm Critic Real\", gradient_norm.mean(), global_step=step)\n",
        "        \n",
        "        #----------------\n",
        "        #we will plot the gradient of critic output with respect to the input image\n",
        "        #get the gradient of the critic output with respect to the input image\n",
        "        gradient = torch.autograd.grad(\n",
        "        inputs=fake,\n",
        "        outputs=critic_fake,\n",
        "        grad_outputs=torch.ones_like(critic_fake),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        )[0]\n",
        "        #flatten the gradient\n",
        "        gradient = gradient.view(gradient.shape[0], -1)\n",
        "        #get the norm of the gradient\n",
        "        gradient_norm = gradient.norm(2, dim=1)\n",
        "        #write gradient norm to tensorboard\n",
        "        writer_loss.add_scalar(\"Gradient norm Critic Fake\", gradient_norm.mean(), global_step=step)\n",
        "        \n",
        "        #----------------\n",
        "        #we will plot the gradient of genrator output with respect to the input \n",
        "        #we will plot the gradient of genrator output with respect to the input \n",
        "        #get the gradient of the generator output with respect to the input noise\n",
        "        gradient = torch.autograd.grad(\n",
        "        inputs=noise,\n",
        "        outputs=gen_fake,\n",
        "        grad_outputs=torch.ones_like(gen_fake),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        )[0]\n",
        "        #flatten the gradient\n",
        "        gradient = gradient.view(gradient.shape[0], -1)\n",
        "        #get the norm of the gradient\n",
        "        gradient_norm = gradient.norm(2, dim=1)\n",
        "        #write gradient norm to tensorboard\n",
        "        writer_loss.add_scalar(\"Gradient norm Generator\", gradient_norm.mean(), global_step=step)\n",
        "        \n",
        "        #----------------\n",
        "\n",
        "        \n",
        "        \n",
        "        # we will plot the gradient penalty\n",
        "        writer_loss.add_scalar(\"GP\", gp, global_step=step)\n",
        "        #we will analyze for vanishing gradient\n",
        "        writer_loss.add_scalar(\"Critic Real\", critic_real.mean(), global_step=step)\n",
        "        writer_loss.add_scalar(\"Critic Fake\", critic_fake.mean(), global_step=step)\n",
        "        \n",
        "        #get the gradient of the critic for the parameters weights of first layer\n",
        "        #we will write the norm of the gardient of weights of the first layer of the critic\n",
        "        for name, param in critic.named_parameters():\n",
        "            if name == \"disc.0.weight\":\n",
        "                writer_loss.add_scalar(\"Critic Gradient w.r.t 1st layer WGAN\", param.grad.norm(), global_step=step)\n",
        "            #also plot the norm of gradient of 2nd layer\n",
        "            elif name == \"disc.2.0.weight\":\n",
        "                writer_loss.add_scalar(\"Critic Gradient w.r.t 2nd layer WGAN\", param.grad.norm(), global_step=step)\n",
        "        \n",
        "       \n",
        "       \n",
        "        \n",
        "        \n",
        "        \n",
        "\n",
        "    step += 1\n",
        "        \n",
        "        #save the trained model\n",
        "        #check if trained_model folder exists\n",
        "    if not os.path.exists(\"trained_models\"):\n",
        "        os.mkdir(\"trained_models\")\n",
        "    \n",
        "    #now trained_model folder exists\n",
        "    if not os.path.exists(\"trained_models/\"+model_name):\n",
        "        os.mkdir(\"trained_models/\"+model_name)\n",
        "    #check if \"trained_models/\"+model_name     \n",
        "    torch.save(gen.state_dict(), \"trained_models/\"+model_name+\"/gen.pth\")\n",
        "    torch.save(critic.state_dict(), \"trained_models/\"+model_name+\"/critic.pth\")\n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "#save the tensorboard\n",
        "writer_real.close()\n",
        "writer_fake.close()\n",
        "writer_loss.close()\n",
        "#calculate the Frchet Inception Distance (FID) to evaluate the performance of the generator\n",
        "\n",
        "#(https://github.com/mseitzer/pytorch-fid)\n",
        "#imports for FID\n",
        "\n",
        "#print generator model architecture\n",
        "print(gen)\n"
      ],
      "metadata": {
        "id": "xLKw9cMGqgmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conditional DCGAN"
      ],
      "metadata": {
        "id": "FTrj1CX-qoN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we will train a conditional wgan on svhn dataset\n",
        "# we will use the gradient penalty to stabilize the training\n",
        "\n",
        "\n",
        "model_name = \"c_wgan7\"\n",
        "#check model saving path is there\n",
        "# imports\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms as transformations\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import PIL.Image as Image\n",
        "import torchvision.models as models\n",
        "#numpy\n",
        "import numpy as np\n",
        "import tqdm\n",
        "from ignite.metrics.gan import FID\n",
        "# hyperparameters for the dataset and the MOdel\n",
        "# Hyperparameters etc.\n",
        "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
        "LEARNING_RATE = 1e-4\n",
        "BATCH_SIZE = 64000\n",
        "IMAGE_SIZE = 64\n",
        "CHANNELS_IMG = 3\n",
        "NUM_CLASSES = 10\n",
        "GEN_EMBEDDING = 100\n",
        "Z_DIM = 100\n",
        "NUM_EPOCHS = 50\n",
        "FEATURES_CRITIC = 16\n",
        "FEATURES_GEN = 16\n",
        "CRITIC_ITERATIONS = 5\n",
        "LAMBDA_GP = 10\n",
        "\n",
        "# prepare the dataset\n",
        "### we will use SVHN dataset for this example\n",
        "### we will combine the train, test and extra datasets to make a bigger dataset\n",
        "transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(IMAGE_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]),\n",
        "    ]\n",
        ")\n",
        "#get the dataset\n",
        "#train part of svhn\n",
        "train_dataset = datasets.SVHN(root=\"dataset_svhm/\", split='train', transform=transforms, download=True)\n",
        "#test part of svhn\n",
        "test_dataset = datasets.SVHN(root=\"dataset_svhm/\", split='test', transform=transforms, download=True)\n",
        "#extra part of svhn\n",
        "extra_dataset = datasets.SVHN(root=\"dataset_svhm/\", split='extra', transform=transforms, download=True)\n",
        "#concatenate the train, test and extra dataset\n",
        "dataset = torch.utils.data.ConcatDataset([train_dataset, test_dataset, extra_dataset])\n",
        "loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "#print the total number of images in the dataset\n",
        "print(len(dataset))\n",
        "# print the shape of the images\n",
        "print(dataset[0][0].shape)\n",
        "# print the label of the image\n",
        "print(dataset[0][1])\n",
        "# Model\n",
        "\n",
        "## generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, channels_noise, channels_img, features_g , num_classes, img_size, embed_size):\n",
        "        super(Generator, self).__init__()\n",
        "        self.img_size = img_size\n",
        "        self.net = nn.Sequential(\n",
        "            # Input: N x channels_noise x 1 x 1\n",
        "            self.block_architecture_generator(channels_noise+embed_size, features_g * 16, 4, 1, 0),  # img: 4x4\n",
        "            self.block_architecture_generator(features_g * 16, features_g * 8, 4, 2, 1),  # img: 8x8\n",
        "            self.block_architecture_generator(features_g * 8, features_g * 4, 4, 2, 1),  # img: 16x16\n",
        "            self.block_architecture_generator(features_g * 4, features_g * 2, 4, 2, 1),  # img: 32x32\n",
        "            nn.ConvTranspose2d(\n",
        "                features_g * 2, channels_img, kernel_size=4, stride=2, padding=1\n",
        "            ),\n",
        "            # Output: N x channels_img x 64 x 64\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "        \n",
        "        self.embed = nn.Embedding(num_classes, embed_size)\n",
        "\n",
        "    def block_architecture_generator(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(\n",
        "                in_channels, out_channels, kernel_size, stride, padding, bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, labels):\n",
        "        embedding = self.embed(labels).unsqueeze(2).unsqueeze(3)\n",
        "        x = torch.cat([x, embedding], 1)\n",
        "        return self.net(x)\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, channels_img, features_d, num_classes, img_size):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.img_size = img_size\n",
        "        self.disc = nn.Sequential(\n",
        "            \n",
        "            nn.Conv2d(channels_img+1, features_d, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # _block(in_channels, out_channels, kernel_size, stride, padding)\n",
        "            self.block_architecture_critic(features_d, features_d * 2, 4, 2, 1),\n",
        "            self.block_architecture_critic(features_d * 2, features_d * 4, 4, 2, 1),\n",
        "            self.block_architecture_critic(features_d * 4, features_d * 8, 4, 2, 1),\n",
        "            # After all _block img output is 4x4 (Conv2d below makes into 1x1)\n",
        "            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0),\n",
        "        )\n",
        "        \n",
        "        #embedding for conditionning\n",
        "        self.embed = nn.Embedding(num_classes, img_size * img_size)\n",
        "\n",
        "    def block_architecture_critic(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels, out_channels, kernel_size, stride, padding, bias=False,\n",
        "            ),\n",
        "            nn.InstanceNorm2d(out_channels, affine=True),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, labels):\n",
        "        embedding = self.embed(labels).view(labels.shape[0], 1, self.img_size, self.img_size)\n",
        "        x = torch.cat([x, embedding], dim=1)\n",
        "        return self.disc(x)\n",
        "--------------------------------\n",
        "# model initialization\n",
        "\n",
        "def initialize_weights(model):\n",
        "    # Initializes weights according to the DCGAN paper\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
        "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "\n",
        "\n",
        "# initialize gen and critic\n",
        "gen = Generator(Z_DIM, CHANNELS_IMG, FEATURES_GEN, NUM_CLASSES, IMAGE_SIZE, GEN_EMBEDDING).to(device)\n",
        "critic = Discriminator(CHANNELS_IMG, FEATURES_CRITIC, NUM_CLASSES, IMAGE_SIZE).to(device)\n",
        "initialize_weights(gen)\n",
        "initialize_weights(critic)\n",
        "### initialize optimizer\n",
        "# initializate optimizer\n",
        "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))\n",
        "opt_critic = optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))\n",
        "### inintialize tensorboard\n",
        "# for tensorboard plotting\n",
        "fixed_noise = torch.randn(100, Z_DIM, 1, 1).to(device)\n",
        "#fixed labels for tensorboard plotting\n",
        "# we will have fixed labels of integers between 0 and 9 for the 10 classes\n",
        "fixed_labels = torch.randint(0, 10, (100,)).to(device)\n",
        "\n",
        "#plot loss of generator and critic\n",
        "writer_loss = SummaryWriter(f\"runs/\"+model_name+\"/loss\")\n",
        "writer_real = SummaryWriter(f\"logs/\"+model_name+\"/real\")\n",
        "writer_fake = SummaryWriter(f\"logs/\"+model_name+\"/fake\")\n",
        "#\n",
        "--------------------------------\n",
        "### initialize FID wrapper\n",
        "fid_score = FID()\n",
        "#interpolate function to resize images to 299,299,3  which is the input size of inception network\n",
        "def interpolate(batch):\n",
        "    arr = []\n",
        "    for img in batch:\n",
        "        pil_img = transformations.ToPILImage()(img)\n",
        "        resized_img = pil_img.resize((299,299), Image.BILINEAR)\n",
        "        arr.append(transformations.ToTensor()(resized_img))\n",
        "    return torch.stack(arr)\n",
        "#TEST FID \n",
        "\n",
        "# y_pred, y = torch.rand(100, 3, 64, 64), torch.rand(100, 3, 64, 64)\n",
        "# y_pred = interpolate(y_pred)\n",
        "# y = interpolate(y)\n",
        "# # m = FID()\n",
        "# fid_score.update((y_pred, y))\n",
        "\n",
        "# print('ignite batch FID', fid_score.compute())  # 8.98434072559458e-05\n",
        "# #reset the fid score\n",
        "# fid_score.reset()\n",
        "## start training\n",
        "\n",
        "\n",
        "gen.train()\n",
        "critic.train()\n",
        "#### gradient penalty function for WGAN-GP\n",
        "def gradient_penalty(critic,labels, real, fake, device=\"cpu\"):\n",
        "    BATCH_SIZE, C, H, W = real.shape\n",
        "    alpha = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
        "    interpolated_images = real * alpha + fake * (1 - alpha)\n",
        "\n",
        "    # Calculate critic scores\n",
        "    mixed_scores = critic(interpolated_images, labels)\n",
        "\n",
        "    # Take the gradient of the scores with respect to the images\n",
        "    gradient = torch.autograd.grad(\n",
        "        inputs=interpolated_images,\n",
        "        outputs=mixed_scores,\n",
        "        grad_outputs=torch.ones_like(mixed_scores),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "    )[0]\n",
        "    gradient = gradient.view(gradient.shape[0], -1)\n",
        "    gradient_norm = gradient.norm(2, dim=1)\n",
        "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
        "    return gradient_penalty\n",
        "step = 0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    #we will track the total loss of the generator and critic for each epoch over the entire dataset\n",
        "    #initialize the total loss of the generator and critic for each epoch to 0\n",
        "    total_loss_gen = 0\n",
        "    total_loss_critic = 0\n",
        "    \n",
        "    #move these to device\n",
        "\n",
        "    #have no gradient for these losse\n",
        "    # total_loss_gen = torch.no_grad()\n",
        "    # total_loss_critic = torch.no_grad()\n",
        "    \n",
        "    # Target labels \n",
        "    for batch_idx, (real, labels) in enumerate(loader):\n",
        "        #send labels to device\n",
        "        labels = labels.to(device)\n",
        "        batch_step = 0\n",
        "        real = real.to(device)\n",
        "        cur_batch_size = real.shape[0]\n",
        "        \n",
        "        \n",
        "\n",
        "        # Train Critic: \n",
        "        for _ in range(CRITIC_ITERATIONS):\n",
        "            noise = torch.randn(cur_batch_size, Z_DIM, 1, 1).to(device)\n",
        "            if len(noise) != len(labels):\n",
        "                noise = noise[:len(labels)]\n",
        "            fake = gen(noise, labels)\n",
        "            critic_real = critic(real, labels).reshape(-1)\n",
        "            critic_fake = critic(fake, labels).reshape(-1)\n",
        "            gp = gradient_penalty(critic, real, fake, device=device)\n",
        "            loss_critic = (\n",
        "                -(torch.mean(critic_real) - torch.mean(critic_fake)) + LAMBDA_GP * gp\n",
        "            )\n",
        "            \n",
        "            \n",
        "            critic.zero_grad()\n",
        "            loss_critic.backward(retain_graph=True)\n",
        "            opt_critic.step()\n",
        "            \n",
        "        #trained critic\n",
        "        \n",
        "\n",
        "        # Train Generator: \n",
        "        gen_fake = critic(fake, labels).reshape(-1)\n",
        "        loss_gen = -torch.mean(gen_fake)\n",
        "        gen.zero_grad()\n",
        "        loss_gen.backward()\n",
        "        opt_gen.step()\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        #update the total loss of the generator and critic for each batch in the epoch\n",
        "        #add just the value no gradients\n",
        "        #have no gradient for these losse\n",
        "        #add just the value no gradientsfrom the loss_gen tensor\n",
        "      \n",
        "        with torch.no_grad():\n",
        "            total_loss_gen += loss_gen.item()\n",
        "            total_loss_critic += loss_critic.item()\n",
        "        \n",
        "        # Print losses occasionally and print to tensorboard in a batch\n",
        "        if batch_idx % 10 == 0 and batch_idx > 0:\n",
        "            print(\n",
        "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(loader)} \\\n",
        "                  Loss D: {loss_critic:.4f}, loss G: {loss_gen:.4f}\"\n",
        "            )\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                \n",
        "                #BATCH LOSS-----\n",
        "                \n",
        "                #write gen_loss and critic_loss to tensorboard\n",
        "                writer_loss.add_scalar(\"Generator loss Batch\", loss_gen, global_step=step)\n",
        "                writer_loss.add_scalar(\"Critic loss Batch\", loss_critic, global_step=step)\n",
        "                \n",
        "                #BATCH LOSS-------\n",
        "\n",
        "                \n",
        "                #FID--\n",
        "                #calculate FID score of this batch\n",
        "                #update the fid_score with real and fake images\n",
        "                real_images_fid = interpolate(real)\n",
        "                fake_images_fid = interpolate(fake)\n",
        "                fid_score.update((real_images_fid, fake_images_fid))\n",
        "                computed_fid_score = fid_score.compute()\n",
        "                print(\"FID score: \", computed_fid_score)\n",
        "                writer_loss.add_scalar(\"FID Score WGAN\", computed_fid_score, global_step=batch_step)\n",
        "                #reset the fid score\n",
        "                fid_score.reset()\n",
        "                ##FID--\n",
        "                \n",
        "                batch_step += 1\n",
        "            \n",
        "            \n",
        "        \n",
        "        \n",
        "\n",
        "        # Print losses occasionally and print to tensorboard per epoch\n",
        "        # if batch_idx % 100 == 0 and batch_idx > 0:\n",
        "        \n",
        "            ## PRINT for few epochs %10\n",
        "            # print(\n",
        "            #     f\"Epoch [{epoch}/{NUM_EPOCHS}]  \\\n",
        "            #         Loss D: {loss_critic:.4f}, loss G: {loss_gen:.4f}\"\n",
        "            # )\n",
        "            \n",
        "    #calculate the Frchet Inception Distance (FID) to evaluate the performance of the generator\n",
        "    \n",
        "\n",
        "    with torch.no_grad():\n",
        "        fake = gen(fixed_noise, fixed_labels)\n",
        "        # take out (up to) 32 examples\n",
        "        img_grid_real = torchvision.utils.make_grid(real[:100], normalize=True)\n",
        "        img_grid_fake = torchvision.utils.make_grid(fake[:100], normalize=True)\n",
        "\n",
        "        writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
        "        writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
        "        \n",
        "        \n",
        "                        \n",
        "        #AVERAGE LOSS---\n",
        "\n",
        "        #get average loss of generator and critic for each epoch\n",
        "        avg_loss_gen = total_loss_gen / len(loader)\n",
        "        avg_loss_critic = total_loss_critic / len(loader)\n",
        "        #write loss to tensorboard\n",
        "        writer_loss.add_scalar(\"Generator loss Epoch\", avg_loss_gen, global_step=batch_step)\n",
        "        writer_loss.add_scalar(\"Critic loss Epoch\", avg_loss_critic, global_step=batch_step)\n",
        "        \n",
        "        #AVERAGE LOSS----\n",
        "        \n",
        "        \n",
        "        \n",
        "        #we will plot the gradient of critic output with respect to the input image\n",
        "        #get the gradient of the critic output with respect to the input image\n",
        "        gradient = torch.autograd.grad(\n",
        "        inputs=real,\n",
        "        outputs=critic_real,\n",
        "        grad_outputs=torch.ones_like(critic_real),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        )[0]\n",
        "        #flatten the gradient\n",
        "        gradient = gradient.view(gradient.shape[0], -1)\n",
        "        #get the norm of the gradient\n",
        "        gradient_norm = gradient.norm(2, dim=1)\n",
        "        #write gradient norm to tensorboard\n",
        "        writer_loss.add_scalar(\"Gradient norm Critic Real\", gradient_norm.mean(), global_step=step)\n",
        "        \n",
        "        #----------------\n",
        "        #we will plot the gradient of critic output with respect to the input image\n",
        "        #get the gradient of the critic output with respect to the input image\n",
        "        gradient = torch.autograd.grad(\n",
        "        inputs=fake,\n",
        "        outputs=critic_fake,\n",
        "        grad_outputs=torch.ones_like(critic_fake),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        )[0]\n",
        "        #flatten the gradient\n",
        "        gradient = gradient.view(gradient.shape[0], -1)\n",
        "        #get the norm of the gradient\n",
        "        gradient_norm = gradient.norm(2, dim=1)\n",
        "        #write gradient norm to tensorboard\n",
        "        writer_loss.add_scalar(\"Gradient norm Critic Fake\", gradient_norm.mean(), global_step=step)\n",
        "        \n",
        "        #----------------\n",
        "        #we will plot the gradient of genrator output with respect to the input \n",
        "        #we will plot the gradient of genrator output with respect to the input \n",
        "        #get the gradient of the generator output with respect to the input noise\n",
        "        gradient = torch.autograd.grad(\n",
        "        inputs=noise,\n",
        "        outputs=gen_fake,\n",
        "        grad_outputs=torch.ones_like(gen_fake),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        )[0]\n",
        "        #flatten the gradient\n",
        "        gradient = gradient.view(gradient.shape[0], -1)\n",
        "        #get the norm of the gradient\n",
        "        gradient_norm = gradient.norm(2, dim=1)\n",
        "        #write gradient norm to tensorboard\n",
        "        writer_loss.add_scalar(\"Gradient norm Generator\", gradient_norm.mean(), global_step=step)\n",
        "        \n",
        "        #----------------\n",
        "\n",
        "        \n",
        "        \n",
        "        # we will plot the gradient penalty\n",
        "        writer_loss.add_scalar(\"GP\", gp, global_step=step)\n",
        "        #we will analyze for vanishing gradient\n",
        "        writer_loss.add_scalar(\"Critic Real\", critic_real.mean(), global_step=step)\n",
        "        writer_loss.add_scalar(\"Critic Fake\", critic_fake.mean(), global_step=step)\n",
        "        \n",
        "        #get the gradient of the critic for the parameters weights of first layer\n",
        "        #we will write the norm of the gardient of weights of the first layer of the critic\n",
        "        for name, param in critic.named_parameters():\n",
        "            if name == \"disc.0.weight\":\n",
        "                writer_loss.add_scalar(\"Critic Gradient w.r.t 1st layer WGAN\", param.grad.norm(), global_step=step)\n",
        "            #also plot the norm of gradient of 2nd layer\n",
        "            elif name == \"disc.2.0.weight\":\n",
        "                writer_loss.add_scalar(\"Critic Gradient w.r.t 2nd layer WGAN\", param.grad.norm(), global_step=step)\n",
        "        \n",
        "       \n",
        "       \n",
        "        \n",
        "        \n",
        "        \n",
        "\n",
        "    step += 1\n",
        "        \n",
        "        #save the trained model\n",
        "        #check if trained_model folder exists\n",
        "    if not os.path.exists(\"trained_models\"):\n",
        "        os.mkdir(\"trained_models\")\n",
        "    \n",
        "    #now trained_model folder exists\n",
        "    if not os.path.exists(\"trained_models/\"+model_name):\n",
        "        os.mkdir(\"trained_models/\"+model_name)\n",
        "    #check if \"trained_models/\"+model_name     \n",
        "    torch.save(gen.state_dict(), \"trained_models/\"+model_name+\"/gen.pth\")\n",
        "    torch.save(critic.state_dict(), \"trained_models/\"+model_name+\"/critic.pth\")\n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "#save the tensorboard\n",
        "writer_real.close()\n",
        "writer_fake.close()\n",
        "writer_loss.close()\n",
        "#calculate the Frchet Inception Distance (FID) to evaluate the performance of the generator\n",
        "\n",
        "#(https://github.com/mseitzer/pytorch-fid)\n",
        "#imports for FID\n",
        "\n",
        "#print generator model architecture\n",
        "print(gen)\n"
      ],
      "metadata": {
        "id": "4eTQWo9mqq5R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}